{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98030af-fcd1-4d63-a36e-38ba053498fa",
   "metadata": {},
   "source": [
    "Creating a product that builds a Brochure for a company to be used for prospective clients, investors and potential recruits.\n",
    "\n",
    "We will be provided a company name and their primary website.\n",
    "\n",
    "See the end of this notebook for examples of real-world business applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b08506-dc8b-4443-9201-5f1848161363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# If these fail, please check you're running from an 'activated' environment with (llms) in the command prompt\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d8880-f2ee-4c06-af16-ecbc0262af61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working!\n"
     ]
    }
   ],
   "source": [
    "# Initialize and constants\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OLLAMA_API_KEY')\n",
    "\n",
    "print(\"Working!\")\n",
    "#if api_key and len(api_key)>10:\n",
    "    #print(\"API key looks good so far\")\n",
    "#else:\n",
    "    #print(\"There might be a problem with your API key?\")\n",
    "    \n",
    "MODEL = 'llama3.2'\n",
    "openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "106dd65e-90af-4ca8-86b6-23a41840645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "    \"\"\"\n",
    "    A utility class to represent a Website that we have scraped, now with links\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            self.text = \"\"\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        self.links = [link for link in links if link]\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30d8128-933b-44cc-81c8-ab4c9d86589a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://edwarddonner.com/',\n",
       " 'https://edwarddonner.com/connect-four/',\n",
       " 'https://edwarddonner.com/outsmart/',\n",
       " 'https://edwarddonner.com/about-me-and-about-nebula/',\n",
       " 'https://edwarddonner.com/posts/',\n",
       " 'https://edwarddonner.com/',\n",
       " 'https://news.ycombinator.com',\n",
       " 'https://nebula.io/?utm_source=ed&utm_medium=referral',\n",
       " 'https://www.prnewswire.com/news-releases/wynden-stark-group-acquires-nyc-venture-backed-tech-startup-untapt-301269512.html',\n",
       " 'https://patents.google.com/patent/US20210049536A1/',\n",
       " 'https://www.linkedin.com/in/eddonner/',\n",
       " 'https://edwarddonner.com/2025/05/28/connecting-my-courses-become-an-llm-expert-and-leader/',\n",
       " 'https://edwarddonner.com/2025/05/28/connecting-my-courses-become-an-llm-expert-and-leader/',\n",
       " 'https://edwarddonner.com/2025/05/18/2025-ai-executive-briefing/',\n",
       " 'https://edwarddonner.com/2025/05/18/2025-ai-executive-briefing/',\n",
       " 'https://edwarddonner.com/2025/04/21/the-complete-agentic-ai-engineering-course/',\n",
       " 'https://edwarddonner.com/2025/04/21/the-complete-agentic-ai-engineering-course/',\n",
       " 'https://edwarddonner.com/2025/01/23/llm-workshop-hands-on-with-agents-resources/',\n",
       " 'https://edwarddonner.com/2025/01/23/llm-workshop-hands-on-with-agents-resources/',\n",
       " 'https://edwarddonner.com/',\n",
       " 'https://edwarddonner.com/connect-four/',\n",
       " 'https://edwarddonner.com/outsmart/',\n",
       " 'https://edwarddonner.com/about-me-and-about-nebula/',\n",
       " 'https://edwarddonner.com/posts/',\n",
       " 'mailto:hello@mygroovydomain.com',\n",
       " 'https://www.linkedin.com/in/eddonner/',\n",
       " 'https://twitter.com/edwarddonner',\n",
       " 'https://www.facebook.com/edward.donner.52']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed = Website(\"https://edwarddonner.com\")\n",
    "ed.links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1771af9c-717a-4fca-bbbe-8a95893312c3",
   "metadata": {},
   "source": [
    "## First step: Have GPT-4o-mini figure out which links are relevant\n",
    "\n",
    "Use a call to gpt-4o-mini to read the links on a webpage, and respond in structured JSON.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6957b079-0d96-45f7-a26a-3487510e9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_system_prompt = \"You are provided with a list of links found on a webpage. \\\n",
    "You are able to decide which of the links would be most relevant to include in a brochure about the company, \\\n",
    "such as links to an About page, or a Company page, or Careers/Jobs pages.\\n\"\n",
    "link_system_prompt += \"You should respond in JSON as in this example:\"\n",
    "link_system_prompt += \"\"\"\n",
    "{\n",
    "    \"links\": [\n",
    "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
    "        {\"type\": \"careers page\", \"url\": \"https://another.full.url/careers\"}\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b97e4068-97ed-4120-beae-c42105e4d59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are provided with a list of links found on a webpage. You are able to decide which of the links would be most relevant to include in a brochure about the company, such as links to an About page, or a Company page, or Careers/Jobs pages.\n",
      "You should respond in JSON as in this example:\n",
      "{\n",
      "    \"links\": [\n",
      "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
      "        {\"type\": \"careers page\", \"url\": \"https://another.full.url/careers\"}\n",
      "    ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(link_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e1f601b-2eaf-499d-b6b8-c99050c9d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_user_prompt(website):\n",
    "    user_prompt = f\"Here is the list of links on the website of {website.url} - \"\n",
    "    user_prompt += \"please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. \\\n",
    "Do not include Terms of Service, Privacy, email links.\\n\"\n",
    "    user_prompt += \"Links (some might be relative links):\\n\"\n",
    "    user_prompt += \"\\n\".join(website.links)\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bcbfa78-6395-4685-b92c-22d592050fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the list of links on the website of https://edwarddonner.com - please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. Do not include Terms of Service, Privacy, email links.\n",
      "Links (some might be relative links):\n",
      "https://edwarddonner.com/\n",
      "https://edwarddonner.com/connect-four/\n",
      "https://edwarddonner.com/outsmart/\n",
      "https://edwarddonner.com/about-me-and-about-nebula/\n",
      "https://edwarddonner.com/posts/\n",
      "https://edwarddonner.com/\n",
      "https://news.ycombinator.com\n",
      "https://nebula.io/?utm_source=ed&utm_medium=referral\n",
      "https://www.prnewswire.com/news-releases/wynden-stark-group-acquires-nyc-venture-backed-tech-startup-untapt-301269512.html\n",
      "https://patents.google.com/patent/US20210049536A1/\n",
      "https://www.linkedin.com/in/eddonner/\n",
      "https://edwarddonner.com/2025/05/28/connecting-my-courses-become-an-llm-expert-and-leader/\n",
      "https://edwarddonner.com/2025/05/28/connecting-my-courses-become-an-llm-expert-and-leader/\n",
      "https://edwarddonner.com/2025/05/18/2025-ai-executive-briefing/\n",
      "https://edwarddonner.com/2025/05/18/2025-ai-executive-briefing/\n",
      "https://edwarddonner.com/2025/04/21/the-complete-agentic-ai-engineering-course/\n",
      "https://edwarddonner.com/2025/04/21/the-complete-agentic-ai-engineering-course/\n",
      "https://edwarddonner.com/2025/01/23/llm-workshop-hands-on-with-agents-resources/\n",
      "https://edwarddonner.com/2025/01/23/llm-workshop-hands-on-with-agents-resources/\n",
      "https://edwarddonner.com/\n",
      "https://edwarddonner.com/connect-four/\n",
      "https://edwarddonner.com/outsmart/\n",
      "https://edwarddonner.com/about-me-and-about-nebula/\n",
      "https://edwarddonner.com/posts/\n",
      "mailto:hello@mygroovydomain.com\n",
      "https://www.linkedin.com/in/eddonner/\n",
      "https://twitter.com/edwarddonner\n",
      "https://www.facebook.com/edward.donner.52\n"
     ]
    }
   ],
   "source": [
    "print(get_links_user_prompt(ed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a29aca19-ca13-471c-a4b4-5abbfa813f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": link_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_links_user_prompt(website)}\n",
    "      ],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    return json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74a827a0-2782-4ae5-b210-4a242a8b4cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/',\n",
       " '/models',\n",
       " '/datasets',\n",
       " '/spaces',\n",
       " '/docs',\n",
       " '/enterprise',\n",
       " '/pricing',\n",
       " '/login',\n",
       " '/join',\n",
       " '/spaces',\n",
       " '/models',\n",
       " '/Qwen/Qwen-Image-Edit',\n",
       " '/deepseek-ai/DeepSeek-V3.1-Base',\n",
       " '/google/gemma-3-270m',\n",
       " '/tencent/Hunyuan-GameCraft-1.0',\n",
       " '/google/gemma-3-270m-it',\n",
       " '/models',\n",
       " '/spaces/enzostvs/deepsite',\n",
       " '/spaces/Qwen/Qwen-Image-Edit',\n",
       " '/spaces/zerogpu-aoti/wan2-2-fp8da-aoti-faster',\n",
       " '/spaces/Qwen/Qwen-Image',\n",
       " '/spaces/AIDC-AI/Ovis2.5-9B',\n",
       " '/spaces',\n",
       " '/datasets/fka/awesome-chatgpt-prompts',\n",
       " '/datasets/nvidia/Granary',\n",
       " '/datasets/nvidia/Llama-Nemotron-VLM-Dataset-v1',\n",
       " '/datasets/allenai/WildChat-4.8M',\n",
       " '/datasets/miromind-ai/MiroVerse-v0.1',\n",
       " '/datasets',\n",
       " '/join',\n",
       " '/pricing#endpoints',\n",
       " '/pricing#spaces',\n",
       " '/pricing',\n",
       " '/enterprise',\n",
       " '/enterprise',\n",
       " '/enterprise',\n",
       " '/enterprise',\n",
       " '/enterprise',\n",
       " '/enterprise',\n",
       " '/enterprise',\n",
       " '/allenai',\n",
       " '/facebook',\n",
       " '/amazon',\n",
       " '/google',\n",
       " '/Intel',\n",
       " '/microsoft',\n",
       " '/grammarly',\n",
       " '/Writer',\n",
       " '/docs/transformers',\n",
       " '/docs/diffusers',\n",
       " '/docs/safetensors',\n",
       " '/docs/huggingface_hub',\n",
       " '/docs/tokenizers',\n",
       " '/docs/trl',\n",
       " '/docs/transformers.js',\n",
       " '/docs/smolagents',\n",
       " '/docs/peft',\n",
       " '/docs/datasets',\n",
       " '/docs/text-generation-inference',\n",
       " '/docs/accelerate',\n",
       " '/models',\n",
       " '/datasets',\n",
       " '/spaces',\n",
       " '/changelog',\n",
       " 'https://endpoints.huggingface.co',\n",
       " '/chat',\n",
       " '/huggingface',\n",
       " '/brand',\n",
       " '/terms-of-service',\n",
       " '/privacy',\n",
       " 'https://apply.workable.com/huggingface/',\n",
       " 'mailto:press@huggingface.co',\n",
       " '/learn',\n",
       " '/docs',\n",
       " '/blog',\n",
       " 'https://discuss.huggingface.co',\n",
       " 'https://status.huggingface.co/',\n",
       " 'https://github.com/huggingface',\n",
       " 'https://twitter.com/huggingface',\n",
       " 'https://www.linkedin.com/company/huggingface/',\n",
       " '/join/discord']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anthropic has made their site harder to scrape, so I'm using HuggingFace..\n",
    "\n",
    "huggingface = Website(\"https://huggingface.co\")\n",
    "huggingface.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d583e2-dcc4-40cc-9b28-1e8dbf402924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'links': [{'type': 'Company info', 'url': 'https://huggingface.co/brand'},\n",
       "  {'type': 'About the company', 'url': 'https://huggingface.co/'},\n",
       "  {'type': 'Join/Discord', 'url': 'https://join.discord.com/huggingface'},\n",
       "  {'type': 'Official blog', 'url': 'https://blog.huggingface.co'},\n",
       "  {'type': 'Support/discussion forum',\n",
       "   'url': 'https://discuss.huggingface.co'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_links(\"https://huggingface.co\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74128e-dfb6-47ec-9549-288b621c838c",
   "metadata": {},
   "source": [
    "## Second step: make the brochure\n",
    "\n",
    "Assemble all the details into another prompt to GPT4-o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a5b6e2-e7ef-44a9-bc7f-59ede71037b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_details(url):\n",
    "    result = \"Landing page:\\n\"\n",
    "    result += Website(url).get_contents()\n",
    "    links = get_links(url)\n",
    "    print(\"Found links:\", links)\n",
    "    for link in links[\"links\"]:\n",
    "        result += f\"\\n\\n{link['type']}\\n\"\n",
    "        result += Website(link[\"url\"]).get_contents()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5099bd14-076d-4745-baf3-dac08d8e5ab2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links: {'links': [{'type': 'about page', 'url': 'https://huggingface.co'}, {'type': 'blog', 'url': 'https://discuss.huggingface.co'}, {'type': 'latest news or updates', 'url': 'https://status.huggingface.co/'}, {'type': 'changelog', 'url': 'https://huggingface.co/changelog'}, {'type': 'Company page', 'url': 'https://huggingface.co/brand'}, {'type': 'GitHub page', 'url': 'https://github.com/huggingface'}, {'type': 'Twitter page', 'url': 'https://twitter.com/huggingface'}, {'type': 'LinkedIn Company page', 'url': 'https://www.linkedin.com/company/huggingface/'}]}\n",
      "Landing page:\n",
      "Webpage Title:\n",
      "Hugging Face ‚Äì The AI community building the future.\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "The AI community building the future.\n",
      "The platform where the machine learning community collaborates on models, datasets, and applications.\n",
      "Explore AI Apps\n",
      "or\n",
      "Browse 1M+ models\n",
      "Trending on\n",
      "this week\n",
      "Models\n",
      "Qwen/Qwen-Image-Edit\n",
      "Updated\n",
      "2 days ago\n",
      "‚Ä¢\n",
      "838\n",
      "deepseek-ai/DeepSeek-V3.1-Base\n",
      "Updated\n",
      "1 day ago\n",
      "‚Ä¢\n",
      "679\n",
      "google/gemma-3-270m\n",
      "Updated\n",
      "7 days ago\n",
      "‚Ä¢\n",
      "6.49k\n",
      "‚Ä¢\n",
      "524\n",
      "tencent/Hunyuan-GameCraft-1.0\n",
      "Updated\n",
      "2 days ago\n",
      "‚Ä¢\n",
      "433\n",
      "google/gemma-3-270m-it\n",
      "Updated\n",
      "7 days ago\n",
      "‚Ä¢\n",
      "9.42k\n",
      "‚Ä¢\n",
      "304\n",
      "Browse 1M+ models\n",
      "Spaces\n",
      "Running\n",
      "12.1k\n",
      "12.1k\n",
      "DeepSite v2\n",
      "üê≥\n",
      "Generate any application with DeepSeek\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "184\n",
      "184\n",
      "Qwen Image Edit\n",
      "‚úí\n",
      "Edit images based on user instructions\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "MCP\n",
      "256\n",
      "256\n",
      "Wan2.2 14B Fast\n",
      "üé•\n",
      "generate a video from an image with a text prompt\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "681\n",
      "681\n",
      "Qwen Image\n",
      "üñº\n",
      "Generate images from text prompts\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "142\n",
      "142\n",
      "Ovis2.5 9B\n",
      "üìä\n",
      "High-accuracy vision & reasoning for complex tasks\n",
      "Browse 400k+ applications\n",
      "Datasets\n",
      "fka/awesome-chatgpt-prompts\n",
      "Updated\n",
      "Jan 6\n",
      "‚Ä¢\n",
      "32.1k\n",
      "‚Ä¢\n",
      "8.78k\n",
      "nvidia/Granary\n",
      "Updated\n",
      "6 days ago\n",
      "‚Ä¢\n",
      "7.75k\n",
      "‚Ä¢\n",
      "96\n",
      "nvidia/Llama-Nemotron-VLM-Dataset-v1\n",
      "Updated\n",
      "1 day ago\n",
      "‚Ä¢\n",
      "2.6k\n",
      "‚Ä¢\n",
      "105\n",
      "allenai/WildChat-4.8M\n",
      "Updated\n",
      "9 days ago\n",
      "‚Ä¢\n",
      "1.37k\n",
      "‚Ä¢\n",
      "80\n",
      "miromind-ai/MiroVerse-v0.1\n",
      "Updated\n",
      "7 days ago\n",
      "‚Ä¢\n",
      "578\n",
      "‚Ä¢\n",
      "57\n",
      "Browse 250k+ datasets\n",
      "The Home of Machine Learning\n",
      "Create, discover and collaborate on ML better.\n",
      "The collaboration platform\n",
      "Host and collaborate on unlimited public models, datasets and applications.\n",
      "Move faster\n",
      "With the HF Open source stack.\n",
      "Explore all modalities\n",
      "Text, image, video, audio or even 3D.\n",
      "Build your portfolio\n",
      "Share your work with the world and build your ML profile.\n",
      "Sign Up\n",
      "Accelerate your ML\n",
      "We provide paid Compute and Enterprise solutions.\n",
      "Compute\n",
      "Deploy on optimized\n",
      "Inference Endpoints\n",
      "or update your\n",
      "Spaces applications\n",
      "to a GPU in a few clicks.\n",
      "View pricing\n",
      "Starting at $0.60/hour for GPU\n",
      "Team & Enterprise\n",
      "Give your team the most advanced platform to build AI with enterprise-grade security, access controls and\n",
      "\t\t\tdedicated support.\n",
      "Getting started\n",
      "Starting at $20/user/month\n",
      "Single Sign-On\n",
      "Regions\n",
      "Priority Support\n",
      "Audit Logs\n",
      "Resource Groups\n",
      "Private Datasets Viewer\n",
      "More than 50,000 organizations are using Hugging Face\n",
      "Ai2\n",
      "Enterprise\n",
      "non-profit\n",
      "‚Ä¢\n",
      "783 models\n",
      "‚Ä¢\n",
      "3.89k followers\n",
      "AI at Meta\n",
      "Enterprise\n",
      "company\n",
      "‚Ä¢\n",
      "2.22k models\n",
      "‚Ä¢\n",
      "7.34k followers\n",
      "Amazon\n",
      "company\n",
      "‚Ä¢\n",
      "20 models\n",
      "‚Ä¢\n",
      "3.37k followers\n",
      "Google\n",
      "Enterprise\n",
      "company\n",
      "‚Ä¢\n",
      "1.04k models\n",
      "‚Ä¢\n",
      "25.4k followers\n",
      "Intel\n",
      "company\n",
      "‚Ä¢\n",
      "241 models\n",
      "‚Ä¢\n",
      "2.9k followers\n",
      "Microsoft\n",
      "company\n",
      "‚Ä¢\n",
      "419 models\n",
      "‚Ä¢\n",
      "14.3k followers\n",
      "Grammarly\n",
      "Team\n",
      "company\n",
      "‚Ä¢\n",
      "11 models\n",
      "‚Ä¢\n",
      "173 followers\n",
      "Writer\n",
      "Enterprise\n",
      "company\n",
      "‚Ä¢\n",
      "21 models\n",
      "‚Ä¢\n",
      "321 followers\n",
      "Our Open Source\n",
      "We are building the foundation of ML tooling with the community.\n",
      "Transformers\n",
      "148,575\n",
      "State-of-the-art AI models for PyTorch\n",
      "Diffusers\n",
      "30,395\n",
      "State-of-the-art Diffusion models in PyTorch\n",
      "Safetensors\n",
      "3,407\n",
      "Safe way to store/distribute neural network weights\n",
      "Hub Python Library\n",
      "2,856\n",
      "Python client to interact with the Hugging Face Hub\n",
      "Tokenizers\n",
      "10,006\n",
      "Fast tokenizers optimized for research & production\n",
      "TRL\n",
      "15,182\n",
      "Train transformers LMs with reinforcement learning\n",
      "Transformers.js\n",
      "14,377\n",
      "State-of-the-art ML running directly in your browser\n",
      "smolagents\n",
      "22,254\n",
      "Smol library to build great agents in Python\n",
      "PEFT\n",
      "19,361\n",
      "Parameter-efficient finetuning for large language models\n",
      "Datasets\n",
      "20,531\n",
      "Access & share datasets for any ML tasks\n",
      "Text Generation Inference\n",
      "10,435\n",
      "Serve language models with TGI optimized toolkit\n",
      "Accelerate\n",
      "9,056\n",
      "Train PyTorch models with multi-GPU, TPU, mixed precision\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Changelog\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "about page\n",
      "Webpage Title:\n",
      "Hugging Face ‚Äì The AI community building the future.\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "The AI community building the future.\n",
      "The platform where the machine learning community collaborates on models, datasets, and applications.\n",
      "Explore AI Apps\n",
      "or\n",
      "Browse 1M+ models\n",
      "Trending on\n",
      "this week\n",
      "Models\n",
      "Qwen/Qwen-Image-Edit\n",
      "Updated\n",
      "2 days ago\n",
      "‚Ä¢\n",
      "838\n",
      "deepseek-ai/DeepSeek-V3.1-Base\n",
      "Updated\n",
      "1 day ago\n",
      "‚Ä¢\n",
      "679\n",
      "google/gemma-3-270m\n",
      "Updated\n",
      "7 days ago\n",
      "‚Ä¢\n",
      "6.49k\n",
      "‚Ä¢\n",
      "524\n",
      "tencent/Hunyuan-GameCraft-1.0\n",
      "Updated\n",
      "2 days ago\n",
      "‚Ä¢\n",
      "433\n",
      "google/gemma-3-270m-it\n",
      "Updated\n",
      "7 days ago\n",
      "‚Ä¢\n",
      "9.42k\n",
      "‚Ä¢\n",
      "304\n",
      "Browse 1M+ models\n",
      "Spaces\n",
      "Running\n",
      "12.1k\n",
      "12.1k\n",
      "DeepSite v2\n",
      "üê≥\n",
      "Generate any application with DeepSeek\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "184\n",
      "184\n",
      "Qwen Image Edit\n",
      "‚úí\n",
      "Edit images based on user instructions\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "MCP\n",
      "257\n",
      "257\n",
      "Wan2.2 14B Fast\n",
      "üé•\n",
      "generate a video from an image with a text prompt\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "681\n",
      "681\n",
      "Qwen Image\n",
      "üñº\n",
      "Generate images from text prompts\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "142\n",
      "142\n",
      "Ovis2.5 9B\n",
      "üìä\n",
      "High-accuracy vision & reasoning for complex tasks\n",
      "Browse 400k+ applications\n",
      "Datasets\n",
      "fka/awesome-chatgpt-prompts\n",
      "Updated\n",
      "Jan 6\n",
      "‚Ä¢\n",
      "32.1k\n",
      "‚Ä¢\n",
      "8.78k\n",
      "nvidia/Granary\n",
      "Updated\n",
      "6 days ago\n",
      "‚Ä¢\n",
      "7.75k\n",
      "‚Ä¢\n",
      "96\n",
      "nvidia/Llama-Nemotron-VLM-Dataset-v1\n",
      "Updated\n",
      "1 day ago\n",
      "‚Ä¢\n",
      "2.6k\n",
      "‚Ä¢\n",
      "105\n",
      "allenai/WildChat-4.8M\n",
      "Updated\n",
      "9 days ago\n",
      "‚Ä¢\n",
      "1.37k\n",
      "‚Ä¢\n",
      "80\n",
      "miromind-ai/MiroVerse-v0.1\n",
      "Updated\n",
      "7 days ago\n",
      "‚Ä¢\n",
      "578\n",
      "‚Ä¢\n",
      "57\n",
      "Browse 250k+ datasets\n",
      "The Home of Machine Learning\n",
      "Create, discover and collaborate on ML better.\n",
      "The collaboration platform\n",
      "Host and collaborate on unlimited public models, datasets and applications.\n",
      "Move faster\n",
      "With the HF Open source stack.\n",
      "Explore all modalities\n",
      "Text, image, video, audio or even 3D.\n",
      "Build your portfolio\n",
      "Share your work with the world and build your ML profile.\n",
      "Sign Up\n",
      "Accelerate your ML\n",
      "We provide paid Compute and Enterprise solutions.\n",
      "Compute\n",
      "Deploy on optimized\n",
      "Inference Endpoints\n",
      "or update your\n",
      "Spaces applications\n",
      "to a GPU in a few clicks.\n",
      "View pricing\n",
      "Starting at $0.60/hour for GPU\n",
      "Team & Enterprise\n",
      "Give your team the most advanced platform to build AI with enterprise-grade security, access controls and\n",
      "\t\t\tdedicated support.\n",
      "Getting started\n",
      "Starting at $20/user/month\n",
      "Single Sign-On\n",
      "Regions\n",
      "Priority Support\n",
      "Audit Logs\n",
      "Resource Groups\n",
      "Private Datasets Viewer\n",
      "More than 50,000 organizations are using Hugging Face\n",
      "Ai2\n",
      "Enterprise\n",
      "non-profit\n",
      "‚Ä¢\n",
      "783 models\n",
      "‚Ä¢\n",
      "3.89k followers\n",
      "AI at Meta\n",
      "Enterprise\n",
      "company\n",
      "‚Ä¢\n",
      "2.22k models\n",
      "‚Ä¢\n",
      "7.34k followers\n",
      "Amazon\n",
      "company\n",
      "‚Ä¢\n",
      "20 models\n",
      "‚Ä¢\n",
      "3.37k followers\n",
      "Google\n",
      "Enterprise\n",
      "company\n",
      "‚Ä¢\n",
      "1.04k models\n",
      "‚Ä¢\n",
      "25.4k followers\n",
      "Intel\n",
      "company\n",
      "‚Ä¢\n",
      "241 models\n",
      "‚Ä¢\n",
      "2.9k followers\n",
      "Microsoft\n",
      "company\n",
      "‚Ä¢\n",
      "419 models\n",
      "‚Ä¢\n",
      "14.3k followers\n",
      "Grammarly\n",
      "Team\n",
      "company\n",
      "‚Ä¢\n",
      "11 models\n",
      "‚Ä¢\n",
      "173 followers\n",
      "Writer\n",
      "Enterprise\n",
      "company\n",
      "‚Ä¢\n",
      "21 models\n",
      "‚Ä¢\n",
      "321 followers\n",
      "Our Open Source\n",
      "We are building the foundation of ML tooling with the community.\n",
      "Transformers\n",
      "148,575\n",
      "State-of-the-art AI models for PyTorch\n",
      "Diffusers\n",
      "30,395\n",
      "State-of-the-art Diffusion models in PyTorch\n",
      "Safetensors\n",
      "3,407\n",
      "Safe way to store/distribute neural network weights\n",
      "Hub Python Library\n",
      "2,856\n",
      "Python client to interact with the Hugging Face Hub\n",
      "Tokenizers\n",
      "10,006\n",
      "Fast tokenizers optimized for research & production\n",
      "TRL\n",
      "15,182\n",
      "Train transformers LMs with reinforcement learning\n",
      "Transformers.js\n",
      "14,377\n",
      "State-of-the-art ML running directly in your browser\n",
      "smolagents\n",
      "22,254\n",
      "Smol library to build great agents in Python\n",
      "PEFT\n",
      "19,361\n",
      "Parameter-efficient finetuning for large language models\n",
      "Datasets\n",
      "20,531\n",
      "Access & share datasets for any ML tasks\n",
      "Text Generation Inference\n",
      "10,435\n",
      "Serve language models with TGI optimized toolkit\n",
      "Accelerate\n",
      "9,056\n",
      "Train PyTorch models with multi-GPU, TPU, mixed precision\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Changelog\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "blog\n",
      "Webpage Title:\n",
      "Hugging Face Forums - Hugging Face Community Discussion\n",
      "Webpage Contents:\n",
      "Hugging Face Forums\n",
      "Topic\n",
      "Replies\n",
      "Views\n",
      "Activity\n",
      "Inference Provider Pricing\n",
      "Beginners\n",
      "6\n",
      "125\n",
      "August 21, 2025\n",
      "RL Course Unit 1: \"python setup.py egg_info did not run successfully\"\n",
      "Beginners\n",
      "2\n",
      "13\n",
      "August 20, 2025\n",
      "Space stuck at ‚ÄúPreparing‚Äù forever ‚Äî no logs, reset doesn‚Äôt work\n",
      "Spaces\n",
      "2\n",
      "23\n",
      "August 20, 2025\n",
      "Can a Model Learn to Generate Better Augmented Data?\n",
      "ü§óTransformers\n",
      "0\n",
      "8\n",
      "August 20, 2025\n",
      "TGI container for fine tuned model?\n",
      "Inference Endpoints on the Hub\n",
      "1\n",
      "3\n",
      "August 21, 2025\n",
      "Seeking Advice: Reliable OCR/AI Pipeline for Extracting Complex Tables from Reports\n",
      "Research\n",
      "3\n",
      "39\n",
      "August 20, 2025\n",
      "Missing dataset card - Reddit-TIFU dataset\n",
      "ü§óDatasets\n",
      "1\n",
      "5\n",
      "August 21, 2025\n",
      "New Project - Echo Nova\n",
      "Intermediate\n",
      "1\n",
      "10\n",
      "August 20, 2025\n",
      "[ERROR] Get error when deploy space\n",
      "Spaces\n",
      "27\n",
      "363\n",
      "August 14, 2025\n",
      "Data storage for pre training Language Model\n",
      "Research\n",
      "2\n",
      "22\n",
      "August 20, 2025\n",
      "Structured ouput with gpt-oss-120b\n",
      "Intermediate\n",
      "1\n",
      "20\n",
      "August 20, 2025\n",
      "How to summarize pdf?\n",
      "Beginners\n",
      "3\n",
      "28\n",
      "August 21, 2025\n",
      "Using datasets.from_generator() with a column that is an embedding\n",
      "Beginners\n",
      "2\n",
      "19\n",
      "August 20, 2025\n",
      "Seed Generator Nodes Missing- COmfyUI-GGUF\n",
      "Beginners\n",
      "1\n",
      "13\n",
      "August 20, 2025\n",
      "From ‚ÄúSem Fundos‚Äù to ‚ÄúI Hate Background‚Äù ‚Äì Evolving a Browser-Based Background Removal Tool\n",
      "Show and Tell\n",
      "3\n",
      "20\n",
      "August 20, 2025\n",
      "Could binary systems make AI text output more efficient?\n",
      "Research\n",
      "2\n",
      "28\n",
      "August 19, 2025\n",
      "Editor Functionality to Community Section\n",
      "Site Feedback\n",
      "1\n",
      "12\n",
      "August 20, 2025\n",
      "[New Dataset Release] Scottish Smallpipes in A (Preview Pack, v0.9)\n",
      "ü§óDatasets\n",
      "0\n",
      "10\n",
      "August 20, 2025\n",
      "First instalment the Muon Optimizer tutorial series\n",
      "Show and Tell\n",
      "2\n",
      "19\n",
      "August 19, 2025\n",
      "Is prometheus-eval not available on HuggingFace Spaces?\n",
      "Beginners\n",
      "1\n",
      "11\n",
      "August 20, 2025\n",
      "Showcasing AERIS: An Experiment in Emergent AI Melancholy\n",
      "Show and Tell\n",
      "9\n",
      "60\n",
      "August 20, 2025\n",
      "Tiny Agent Mcp course- connection close\n",
      "Course\n",
      "1\n",
      "10\n",
      "August 20, 2025\n",
      "LORA - how to determine what module_to_save\n",
      "Beginners\n",
      "2\n",
      "22\n",
      "August 19, 2025\n",
      "Complaint regarding three $10 charges following a rejected transaction\n",
      "Inference Endpoints on the Hub\n",
      "2\n",
      "19\n",
      "August 20, 2025\n",
      "403 Forbidden Error ‚Äì Cannot Use Storage\n",
      "Models\n",
      "2\n",
      "29\n",
      "August 20, 2025\n",
      "ChatGPT 5 An Initial Review\n",
      "Research\n",
      "17\n",
      "577\n",
      "August 14, 2025\n",
      "AutoTrain LLM on Spaces: OMP_NUM_THREADS ValueError: '3500m' despite explicit ENV var setting\n",
      "ü§óAutoTrain\n",
      "4\n",
      "54\n",
      "August 20, 2025\n",
      "Why does DDPMPipeline.from_pretrained() fail to execute, while passing the same parameters to UNet2DModel.from_pretrained() works fine?\n",
      "Beginners\n",
      "2\n",
      "16\n",
      "August 19, 2025\n",
      "WFGY 2.0 ‚Äî My Seven-Step Reasoning Engine (for the open-source community)\n",
      "Beginners\n",
      "1\n",
      "28\n",
      "August 19, 2025\n",
      "Is it possible to fine tune a Local LLM pulled from Ollama to perform toolcalling/function calling better!\n",
      "Beginners\n",
      "1\n",
      "21\n",
      "August 19, 2025\n",
      "next page ‚Üí\n",
      "Home\n",
      "Categories\n",
      "Guidelines\n",
      "Terms of Service\n",
      "Privacy Policy\n",
      "Powered by\n",
      "Discourse\n",
      ", best viewed with JavaScript enabled\n",
      "\n",
      "\n",
      "\n",
      "latest news or updates\n",
      "Webpage Title:\n",
      "\n",
      "Hugging Face status\n",
      "\n",
      "Webpage Contents:\n",
      "Status\n",
      "Maintenance\n",
      "Previous incidents\n",
      "Get updates\n",
      "Get status updates\n",
      "E-mail\n",
      "RSS\n",
      "JSON\n",
      "Webhook\n",
      "Get e-mail notifications whenever Hugging Face creates, updates or resolves an incident.\n",
      "Subscribe to specific components\n",
      "Current status by service\n",
      "Huggingface Hub\n",
      "Git Hosting and Serving\n",
      "Inference Endpoints\n",
      "Inference Endpoints UI\n",
      "Inference Endpoints API\n",
      "Spaces\n",
      "Spaces Proxy\n",
      "Subscribe\n",
      "Get the RSS feed\n",
      "Get the entire status page as JSON\n",
      "Get a request to your URL whenever Hugging Face creates, updates or resolves an incident.\n",
      "We'll email you with confirmation and when there's an issue with your URL\n",
      "Subscribe to specific components\n",
      "Current status by service\n",
      "Huggingface Hub\n",
      "Git Hosting and Serving\n",
      "Inference Endpoints\n",
      "Inference Endpoints UI\n",
      "Inference Endpoints API\n",
      "Spaces\n",
      "Spaces Proxy\n",
      "Subscribe\n",
      "All services are online\n",
      "Last updated on Aug 20 at 10:19pm EDT\n",
      "Current status by service\n",
      "Operational\n",
      "Huggingface Hub\n",
      "99.960% uptime\n",
      "Operational\n",
      "May 23, 2025\n",
      "Operational\n",
      "May 24, 2025\n",
      "Operational\n",
      "May 25, 2025\n",
      "Operational\n",
      "May 26, 2025\n",
      "Operational\n",
      "May 27, 2025\n",
      "Operational\n",
      "May 28, 2025\n",
      "Operational\n",
      "May 29, 2025\n",
      "Operational\n",
      "May 30, 2025\n",
      "Operational\n",
      "May 31, 2025\n",
      "Operational\n",
      "Jun 01, 2025\n",
      "Operational\n",
      "Jun 02, 2025\n",
      "Operational\n",
      "Jun 03, 2025\n",
      "Operational\n",
      "Jun 04, 2025\n",
      "Operational\n",
      "Jun 05, 2025\n",
      "Operational\n",
      "Jun 06, 2025\n",
      "Operational\n",
      "Jun 07, 2025\n",
      "Downtime\n",
      "Down for 4¬†minutes\n",
      "Jun 08, 2025\n",
      "Operational\n",
      "Jun 09, 2025\n",
      "Operational\n",
      "Jun 10, 2025\n",
      "Operational\n",
      "Jun 11, 2025\n",
      "Operational\n",
      "Jun 12, 2025\n",
      "Operational\n",
      "Jun 13, 2025\n",
      "Downtime\n",
      "Down for 33¬†minutes\n",
      "Jun 14, 2025\n",
      "Operational\n",
      "Jun 15, 2025\n",
      "Downtime\n",
      "Down for 5¬†minutes\n",
      "Jun 16, 2025\n",
      "Operational\n",
      "Jun 17, 2025\n",
      "Operational\n",
      "Jun 18, 2025\n",
      "Downtime\n",
      "Down for 4¬†minutes\n",
      "Jun 19, 2025\n",
      "Operational\n",
      "Jun 20, 2025\n",
      "Operational\n",
      "Jun 21, 2025\n",
      "Operational\n",
      "Jun 22, 2025\n",
      "Operational\n",
      "Jun 23, 2025\n",
      "Downtime\n",
      "Down for 4¬†minutes\n",
      "Jun 24, 2025\n",
      "Operational\n",
      "Jun 25, 2025\n",
      "Operational\n",
      "Jun 26, 2025\n",
      "Operational\n",
      "Jun 27, 2025\n",
      "Operational\n",
      "Jun 28, 2025\n",
      "Operational\n",
      "Jun 29, 2025\n",
      "Operational\n",
      "Jun 30, 2025\n",
      "Operational\n",
      "Jul 01, 2025\n",
      "Operational\n",
      "Jul 02, 2025\n",
      "Operational\n",
      "Jul 03, 2025\n",
      "Operational\n",
      "Jul 04, 2025\n",
      "Operational\n",
      "Jul 05, 2025\n",
      "Operational\n",
      "Jul 06, 2025\n",
      "Operational\n",
      "Jul 07, 2025\n",
      "Operational\n",
      "Jul 08, 2025\n",
      "Operational\n",
      "Jul 09, 2025\n",
      "Operational\n",
      "Jul 10, 2025\n",
      "Operational\n",
      "Jul 11, 2025\n",
      "Operational\n",
      "Jul 12, 2025\n",
      "Operational\n",
      "Jul 13, 2025\n",
      "Operational\n",
      "Jul 14, 2025\n",
      "Operational\n",
      "Jul 15, 2025\n",
      "Operational\n",
      "Jul 16, 2025\n",
      "Operational\n",
      "Jul 17, 2025\n",
      "Operational\n",
      "Jul 18, 2025\n",
      "Operational\n",
      "Jul 19, 2025\n",
      "Operational\n",
      "Jul 20, 2025\n",
      "Operational\n",
      "Jul 21, 2025\n",
      "Operational\n",
      "Jul 22, 2025\n",
      "Operational\n",
      "Jul 23, 2025\n",
      "Operational\n",
      "Jul 24, 2025\n",
      "Operational\n",
      "Jul 25, 2025\n",
      "Operational\n",
      "Jul 26, 2025\n",
      "Operational\n",
      "Jul 27, 2025\n",
      "Operational\n",
      "Jul 28, 2025\n",
      "Operational\n",
      "Jul 29, 2025\n",
      "Operational\n",
      "Jul 30, 2025\n",
      "Operational\n",
      "Jul 31, 2025\n",
      "Operational\n",
      "Aug 01, 2025\n",
      "Operational\n",
      "Aug 02, 2025\n",
      "Operational\n",
      "Aug 03, 2025\n",
      "Operational\n",
      "Aug 04, 2025\n",
      "Operational\n",
      "Aug 05, 2025\n",
      "Operational\n",
      "Aug 06, 2025\n",
      "Operational\n",
      "Aug 07, 2025\n",
      "Operational\n",
      "Aug 08, 2025\n",
      "Operational\n",
      "Aug 09, 2025\n",
      "Operational\n",
      "Aug 10, 2025\n",
      "Operational\n",
      "Aug 11, 2025\n",
      "Operational\n",
      "Aug 12, 2025\n",
      "Operational\n",
      "Aug 13, 2025\n",
      "Operational\n",
      "Aug 14, 2025\n",
      "Operational\n",
      "Aug 15, 2025\n",
      "Operational\n",
      "Aug 16, 2025\n",
      "Operational\n",
      "Aug 17, 2025\n",
      "Operational\n",
      "Aug 18, 2025\n",
      "Operational\n",
      "Aug 19, 2025\n",
      "Operational\n",
      "Aug 20, 2025\n",
      "30 days ago\n",
      "60 days ago\n",
      "90 days ago\n",
      "Today\n",
      "Git Hosting and Serving\n",
      "99.928% uptime\n",
      "Operational\n",
      "May 23, 2025\n",
      "Operational\n",
      "May 24, 2025\n",
      "Operational\n",
      "May 25, 2025\n",
      "Operational\n",
      "May 26, 2025\n",
      "Operational\n",
      "May 27, 2025\n",
      "Operational\n",
      "May 28, 2025\n",
      "Operational\n",
      "May 29, 2025\n",
      "Operational\n",
      "May 30, 2025\n",
      "Operational\n",
      "May 31, 2025\n",
      "Operational\n",
      "Jun 01, 2025\n",
      "Operational\n",
      "Jun 02, 2025\n",
      "Operational\n",
      "Jun 03, 2025\n",
      "Operational\n",
      "Jun 04, 2025\n",
      "Operational\n",
      "Jun 05, 2025\n",
      "Downtime\n",
      "Down for 6¬†minutes\n",
      "Jun 06, 2025\n",
      "Operational\n",
      "Jun 07, 2025\n",
      "Downtime\n",
      "Down for 33¬†minutes\n",
      "Jun 08, 2025\n",
      "Operational\n",
      "Jun 09, 2025\n",
      "Operational\n",
      "Jun 10, 2025\n",
      "Operational\n",
      "Jun 11, 2025\n",
      "Operational\n",
      "Jun 12, 2025\n",
      "Operational\n",
      "Jun 13, 2025\n",
      "Downtime\n",
      "Down for 33¬†minutes\n",
      "Jun 14, 2025\n",
      "Operational\n",
      "Jun 15, 2025\n",
      "Operational\n",
      "Jun 16, 2025\n",
      "Operational\n",
      "Jun 17, 2025\n",
      "Downtime\n",
      "Down for 4¬†minutes\n",
      "Jun 18, 2025\n",
      "Operational\n",
      "Jun 19, 2025\n",
      "Operational\n",
      "Jun 20, 2025\n",
      "Operational\n",
      "Jun 21, 2025\n",
      "Operational\n",
      "Jun 22, 2025\n",
      "Operational\n",
      "Jun 23, 2025\n",
      "Downtime\n",
      "Down for 10¬†minutes\n",
      "Jun 24, 2025\n",
      "Operational\n",
      "Jun 25, 2025\n",
      "Downtime\n",
      "Down for 4¬†minutes\n",
      "Jun 26, 2025\n",
      "Operational\n",
      "Jun 27, 2025\n",
      "Operational\n",
      "Jun 28, 2025\n",
      "Operational\n",
      "Jun 29, 2025\n",
      "Operational\n",
      "Jun 30, 2025\n",
      "Operational\n",
      "Jul 01, 2025\n",
      "Operational\n",
      "Jul 02, 2025\n",
      "Operational\n",
      "Jul 03, 2025\n",
      "Operational\n",
      "Jul 04, 2025\n",
      "Operational\n",
      "Jul 05, 2025\n",
      "Operational\n",
      "Jul 06, 2025\n",
      "Operational\n",
      "Jul 07, 2025\n",
      "Operational\n",
      "Jul 08, 2025\n",
      "Operational\n",
      "Jul 09, 2025\n",
      "Operational\n",
      "Jul 10, 2025\n",
      "Operational\n",
      "Jul 11, 2025\n",
      "Operational\n",
      "Jul 12, 2025\n",
      "Operational\n",
      "Jul 13, 2025\n",
      "Operational\n",
      "Jul 14, 2025\n",
      "Operational\n",
      "Jul 15, 2025\n",
      "Operational\n",
      "Jul 16, 2025\n",
      "Operational\n",
      "Jul 17, 2025\n",
      "Operational\n",
      "Jul 18, 2025\n",
      "Operational\n",
      "Jul 19, 2025\n",
      "Operational\n",
      "Jul 20, 2025\n",
      "Operational\n",
      "Jul 21, 2025\n",
      "Operational\n",
      "Jul 22, 2025\n",
      "Operational\n",
      "Jul 23, 2025\n",
      "Operational\n",
      "Jul 24, 2025\n",
      "Operational\n",
      "Jul 25, 2025\n",
      "Operational\n",
      "Jul 26, 2025\n",
      "Operational\n",
      "Jul 27, 2025\n",
      "Operational\n",
      "Jul 28, 2025\n",
      "Operational\n",
      "Jul 29, 2025\n",
      "Operational\n",
      "Jul 30, 2025\n",
      "Operational\n",
      "Jul 31, 2025\n",
      "Operational\n",
      "Aug 01, 2025\n",
      "Operational\n",
      "Aug 02, 2025\n",
      "Operational\n",
      "Aug 03, 2025\n",
      "Operational\n",
      "Aug 04, 2025\n",
      "Operational\n",
      "Aug 05, 2025\n",
      "Operational\n",
      "Aug 06, 2025\n",
      "Operational\n",
      "Aug 07, 2025\n",
      "Operational\n",
      "Aug 08, 2025\n",
      "Operational\n",
      "Aug 09, 2025\n",
      "Operational\n",
      "Aug 10, 2025\n",
      "Operational\n",
      "Aug 11, 2025\n",
      "Operational\n",
      "Aug 12, 2025\n",
      "Operational\n",
      "Aug 13, 2025\n",
      "Operational\n",
      "Aug 14, 2025\n",
      "Operational\n",
      "Aug 15, 2025\n",
      "Operational\n",
      "Aug 16, 2025\n",
      "Operational\n",
      "Aug 17, 2025\n",
      "Operational\n",
      "Aug 18, 2025\n",
      "Operational\n",
      "Aug 19, 2025\n",
      "Operational\n",
      "Aug 20, 2025\n",
      "30 days ago\n",
      "60 days ago\n",
      "90 days ago\n",
      "Today\n",
      "Inference Endpoints\n",
      "Operational\n",
      "Inference Endpoints UI\n",
      "99.969% uptime\n",
      "Operational\n",
      "May 23, 2025\n",
      "Operational\n",
      "May 24, 2025\n",
      "Operational\n",
      "May 25, 2025\n",
      "Operational\n",
      "May 26, 2025\n",
      "Operational\n",
      "May 27, 2025\n",
      "Operational\n",
      "May 28, 2025\n",
      "Operational\n",
      "May 29, 2025\n",
      "Operational\n",
      "May 30, 2025\n",
      "Operational\n",
      "May 31, 2025\n",
      "Operational\n",
      "Jun 01, 2025\n",
      "Operational\n",
      "Jun 02, 2025\n",
      "Operational\n",
      "Jun 03, 2025\n",
      "Operational\n",
      "Jun 04, 2025\n",
      "Operational\n",
      "Jun 05, 2025\n",
      "Operational\n",
      "Jun 06, 2025\n",
      "Operational\n",
      "Jun 07, 2025\n",
      "Operational\n",
      "Jun 08, 2025\n",
      "Operational\n",
      "Jun 09, 2025\n",
      "Operational\n",
      "Jun 10, 2025\n",
      "Operational\n",
      "Jun 11, 2025\n",
      "Downtime\n",
      "Down for 19¬†minutes\n",
      "Jun 12, 2025\n",
      "Operational\n",
      "Jun 13, 2025\n",
      "Operational\n",
      "Jun 14, 2025\n",
      "Operational\n",
      "Jun 15, 2025\n",
      "Operational\n",
      "Jun 16, 2025\n",
      "Operational\n",
      "Jun 17, 2025\n",
      "Operational\n",
      "Jun 18, 2025\n",
      "Operational\n",
      "Jun 19, 2025\n",
      "Downtime\n",
      "Down for 20¬†minutes\n",
      "Jun 20, 2025\n",
      "Operational\n",
      "Jun 21, 2025\n",
      "Operational\n",
      "Jun 22, 2025\n",
      "Operational\n",
      "Jun 23, 2025\n",
      "Operational\n",
      "Jun 24, 2025\n",
      "Operational\n",
      "Jun 25, 2025\n",
      "Operational\n",
      "Jun 26, 2025\n",
      "Operational\n",
      "Jun 27, 2025\n",
      "Operational\n",
      "Jun 28, 2025\n",
      "Operational\n",
      "Jun 29, 2025\n",
      "Operational\n",
      "Jun 30, 2025\n",
      "Operational\n",
      "Jul 01, 2025\n",
      "Operational\n",
      "Jul 02, 2025\n",
      "Operational\n",
      "Jul 03, 2025\n",
      "Operational\n",
      "Jul 04, 2025\n",
      "Operational\n",
      "Jul 05, 2025\n",
      "Operational\n",
      "Jul 06, 2025\n",
      "Operational\n",
      "Jul 07, 2025\n",
      "Operational\n",
      "Jul 08, 2025\n",
      "Operational\n",
      "Jul 09, 2025\n",
      "Operational\n",
      "Jul 10, 2025\n",
      "Operational\n",
      "Jul 11, 2025\n",
      "Operational\n",
      "Jul 12, 2025\n",
      "Operational\n",
      "Jul 13, 2025\n",
      "Operational\n",
      "Jul 14, 2025\n",
      "Operational\n",
      "Jul 15, 2025\n",
      "Operational\n",
      "Jul 16, 2025\n",
      "Operational\n",
      "Jul 17, 2025\n",
      "Operational\n",
      "Jul 18, 2025\n",
      "Operational\n",
      "Jul 19, 2025\n",
      "Operational\n",
      "Jul 20, 2025\n",
      "Operational\n",
      "Jul 21, 2025\n",
      "Operational\n",
      "Jul 22, 2025\n",
      "Operational\n",
      "Jul 23, 2025\n",
      "Operational\n",
      "Jul 24, 2025\n",
      "Operational\n",
      "Jul 25, 2025\n",
      "Operational\n",
      "Jul 26, 2025\n",
      "Operational\n",
      "Jul 27, 2025\n",
      "Operational\n",
      "Jul 28, 2025\n",
      "Operational\n",
      "Jul 29, 2025\n",
      "Operational\n",
      "Jul 30, 2025\n",
      "Operational\n",
      "Jul 31, 2025\n",
      "Operational\n",
      "Aug 01, 2025\n",
      "Operational\n",
      "Aug 02, 2025\n",
      "Operational\n",
      "Aug 03, 2025\n",
      "Operational\n",
      "Aug 04, 2025\n",
      "Operational\n",
      "Aug 05, 2025\n",
      "Operational\n",
      "Aug 06, 2025\n",
      "Operational\n",
      "Aug 07, 2025\n",
      "Operational\n",
      "Aug 08, 2025\n",
      "Operational\n",
      "Aug 09, 2025\n",
      "Operational\n",
      "Aug 10, 2025\n",
      "Operational\n",
      "Aug 11, 2025\n",
      "Operational\n",
      "Aug 12, 2025\n",
      "Operational\n",
      "Aug 13, 2025\n",
      "Operational\n",
      "Aug 14, 2025\n",
      "Operational\n",
      "Aug 15, 2025\n",
      "Operational\n",
      "Aug 16, 2025\n",
      "Operational\n",
      "Aug 17, 2025\n",
      "Operational\n",
      "Aug 18, 2025\n",
      "Operational\n",
      "Aug 19, 2025\n",
      "Operational\n",
      "Aug 20, 2025\n",
      "30 days ago\n",
      "60 days ago\n",
      "90 days ago\n",
      "Today\n",
      "Inference Endpoints API\n",
      "100.000% uptime\n",
      "Operational\n",
      "May 23, 2025\n",
      "Operational\n",
      "May 24, 2025\n",
      "Operational\n",
      "May 25, 2025\n",
      "Operational\n",
      "May 26, 2025\n",
      "Operational\n",
      "May 27, 2025\n",
      "Operational\n",
      "May 28, 2025\n",
      "Operational\n",
      "May 29, 2025\n",
      "Operational\n",
      "May 30, 2025\n",
      "Operational\n",
      "May 31, 2025\n",
      "Operational\n",
      "Jun 01, 2025\n",
      "Operational\n",
      "Jun 02, 2025\n",
      "Operational\n",
      "Jun 03, 2025\n",
      "Operational\n",
      "Jun 04, 2025\n",
      "Operational\n",
      "Jun 05, 2025\n",
      "Operational\n",
      "Jun 06, 2025\n",
      "Operational\n",
      "Jun 07, 2025\n",
      "Operational\n",
      "Jun 08, 2025\n",
      "Operational\n",
      "Jun 09, 2025\n",
      "Operational\n",
      "Jun 10, 2025\n",
      "Operational\n",
      "Jun 11, 2025\n",
      "Operational\n",
      "Jun 12, 2025\n",
      "Operational\n",
      "Jun 13, 2025\n",
      "Operational\n",
      "Jun 14, 2025\n",
      "Operational\n",
      "Jun 15, 2025\n",
      "Operational\n",
      "Jun 16, 2025\n",
      "Operational\n",
      "Jun 17, 2025\n",
      "Operational\n",
      "Jun 18, 2025\n",
      "Operational\n",
      "Jun 19, 2025\n",
      "Operational\n",
      "Jun 20, 2025\n",
      "Operational\n",
      "Jun 21, 2025\n",
      "Operational\n",
      "Jun 22, 2025\n",
      "Operational\n",
      "Jun 23, 2025\n",
      "Operational\n",
      "Jun 24, 2025\n",
      "Operational\n",
      "Jun 25, 2025\n",
      "Operational\n",
      "Jun 26, 2025\n",
      "Operational\n",
      "Jun 27, 2025\n",
      "Operational\n",
      "Jun 28, 2025\n",
      "Operational\n",
      "Jun 29, 2025\n",
      "Operational\n",
      "Jun 30, 2025\n",
      "Operational\n",
      "Jul 01, 2025\n",
      "Operational\n",
      "Jul 02, 2025\n",
      "Operational\n",
      "Jul 03, 2025\n",
      "Operational\n",
      "Jul 04, 2025\n",
      "Operational\n",
      "Jul 05, 2025\n",
      "Operational\n",
      "Jul 06, 2025\n",
      "Operational\n",
      "Jul 07, 2025\n",
      "Operational\n",
      "Jul 08, 2025\n",
      "Operational\n",
      "Jul 09, 2025\n",
      "Operational\n",
      "Jul 10, 2025\n",
      "Operational\n",
      "Jul 11, 2025\n",
      "Operational\n",
      "Jul 12, 2025\n",
      "Operational\n",
      "Jul 13, 2025\n",
      "Operational\n",
      "Jul 14, 2025\n",
      "Operational\n",
      "Jul 15, 2025\n",
      "Operational\n",
      "Jul 16, 2025\n",
      "Operational\n",
      "Jul 17, 2025\n",
      "Operational\n",
      "Jul 18, 2025\n",
      "Operational\n",
      "Jul 19, 2025\n",
      "Operational\n",
      "Jul 20, 2025\n",
      "Operational\n",
      "Jul 21, 2025\n",
      "Operational\n",
      "Jul 22, 2025\n",
      "Operational\n",
      "Jul 23, 2025\n",
      "Operational\n",
      "Jul 24, 2025\n",
      "Operational\n",
      "Jul 25, 2025\n",
      "Operational\n",
      "Jul 26, 2025\n",
      "Operational\n",
      "Jul 27, 2025\n",
      "Operational\n",
      "Jul 28, 2025\n",
      "Operational\n",
      "Jul 29, 2025\n",
      "Operational\n",
      "Jul 30, 2025\n",
      "Operational\n",
      "Jul 31, 2025\n",
      "Operational\n",
      "Aug 01, 2025\n",
      "Operational\n",
      "Aug 02, 2025\n",
      "Operational\n",
      "Aug 03, 2025\n",
      "Operational\n",
      "Aug 04, 2025\n",
      "Operational\n",
      "Aug 05, 2025\n",
      "Operational\n",
      "Aug 06, 2025\n",
      "Operational\n",
      "Aug 07, 2025\n",
      "Operational\n",
      "Aug 08, 2025\n",
      "Operational\n",
      "Aug 09, 2025\n",
      "Operational\n",
      "Aug 10, 2025\n",
      "Operational\n",
      "Aug 11, 2025\n",
      "Operational\n",
      "Aug 12, 2025\n",
      "Operational\n",
      "Aug 13, 2025\n",
      "Operational\n",
      "Aug 14, 2025\n",
      "Operational\n",
      "Aug 15, 2025\n",
      "Operational\n",
      "Aug 16, 2025\n",
      "Operational\n",
      "Aug 17, 2025\n",
      "Operational\n",
      "Aug 18, 2025\n",
      "Operational\n",
      "Aug 19, 2025\n",
      "Operational\n",
      "Aug 20, 2025\n",
      "30 days ago\n",
      "60 days ago\n",
      "90 days ago\n",
      "Today\n",
      "Spaces\n",
      "Operational\n",
      "Spaces Proxy\n",
      "100.000% uptime\n",
      "Operational\n",
      "May 23, 2025\n",
      "Operational\n",
      "May 24, 2025\n",
      "Operational\n",
      "May 25, 2025\n",
      "Operational\n",
      "May 26, 2025\n",
      "Operational\n",
      "May 27, 2025\n",
      "Operational\n",
      "May 28, 2025\n",
      "Operational\n",
      "May 29, 2025\n",
      "Operational\n",
      "May 30, 2025\n",
      "Operational\n",
      "May 31, 2025\n",
      "Operational\n",
      "Jun 01, 2025\n",
      "Operational\n",
      "Jun 02, 2025\n",
      "Operational\n",
      "Jun 03, 2025\n",
      "Operational\n",
      "Jun 04, 2025\n",
      "Operational\n",
      "Jun 05, 2025\n",
      "Operational\n",
      "Jun 06, 2025\n",
      "Operational\n",
      "Jun 07, 2025\n",
      "Operational\n",
      "Jun 08, 2025\n",
      "Operational\n",
      "Jun 09, 2025\n",
      "Operational\n",
      "Jun 10, 2025\n",
      "Operational\n",
      "Jun 11, 2025\n",
      "Operational\n",
      "Jun 12, 2025\n",
      "Operational\n",
      "Jun 13, 2025\n",
      "Operational\n",
      "Jun 14, 2025\n",
      "Operational\n",
      "Jun 15, 2025\n",
      "Operational\n",
      "Jun 16, 2025\n",
      "Operational\n",
      "Jun 17, 2025\n",
      "Operational\n",
      "Jun 18, 2025\n",
      "Operational\n",
      "Jun 19, 2025\n",
      "Operational\n",
      "Jun 20, 2025\n",
      "Operational\n",
      "Jun 21, 2025\n",
      "Operational\n",
      "Jun 22, 2025\n",
      "Operational\n",
      "Jun 23, 2025\n",
      "Operational\n",
      "Jun 24, 2025\n",
      "Operational\n",
      "Jun 25, 2025\n",
      "Operational\n",
      "Jun 26, 2025\n",
      "Operational\n",
      "Jun 27, 2025\n",
      "Operational\n",
      "Jun 28, 2025\n",
      "Operational\n",
      "Jun 29, 2025\n",
      "Operational\n",
      "Jun 30, 2025\n",
      "Operational\n",
      "Jul 01, 2025\n",
      "Operational\n",
      "Jul 02, 2025\n",
      "Operational\n",
      "Jul 03, 2025\n",
      "Operational\n",
      "Jul 04, 2025\n",
      "Operational\n",
      "Jul 05, 2025\n",
      "Operational\n",
      "Jul 06, 2025\n",
      "Operational\n",
      "Jul 07, 2025\n",
      "Operational\n",
      "Jul 08, 2025\n",
      "Operational\n",
      "Jul 09, 2025\n",
      "Operational\n",
      "Jul 10, 2025\n",
      "Operational\n",
      "Jul 11, 2025\n",
      "Operational\n",
      "Jul 12, 2025\n",
      "Operational\n",
      "Jul 13, 2025\n",
      "Operational\n",
      "Jul 14, 2025\n",
      "Operational\n",
      "Jul 15, 2025\n",
      "Operational\n",
      "Jul 16, 2025\n",
      "Operational\n",
      "Jul 17, 2025\n",
      "Operational\n",
      "Jul 18, 2025\n",
      "Operational\n",
      "Jul 19, 2025\n",
      "Operational\n",
      "Jul 20, 2025\n",
      "Operational\n",
      "Jul 21, 2025\n",
      "Operational\n",
      "Jul 22, 2025\n",
      "Operational\n",
      "Jul 23, 2025\n",
      "Operational\n",
      "Jul 24, 2025\n",
      "Operational\n",
      "Jul 25, 2025\n",
      "Operational\n",
      "Jul 26, 2025\n",
      "Operational\n",
      "Jul 27, 2025\n",
      "Operational\n",
      "Jul 28, 2025\n",
      "Operational\n",
      "Jul 29, 2025\n",
      "Operational\n",
      "Jul 30, 2025\n",
      "Operational\n",
      "Jul 31, 2025\n",
      "Operational\n",
      "Aug 01, 2025\n",
      "Operational\n",
      "Aug 02, 2025\n",
      "Operational\n",
      "Aug 03, 2025\n",
      "Operational\n",
      "Aug 04, 2025\n",
      "Operational\n",
      "Aug 05, 2025\n",
      "Operational\n",
      "Aug 06, 2025\n",
      "Operational\n",
      "Aug 07, 2025\n",
      "Operational\n",
      "Aug 08, 2025\n",
      "Operational\n",
      "Aug 09, 2025\n",
      "Operational\n",
      "Aug 10, 2025\n",
      "Operational\n",
      "Aug 11, 2025\n",
      "Operational\n",
      "Aug 12, 2025\n",
      "Operational\n",
      "Aug 13, 2025\n",
      "Operational\n",
      "Aug 14, 2025\n",
      "Operational\n",
      "Aug 15, 2025\n",
      "Operational\n",
      "Aug 16, 2025\n",
      "Operational\n",
      "Aug 17, 2025\n",
      "Operational\n",
      "Aug 18, 2025\n",
      "Operational\n",
      "Aug 19, 2025\n",
      "Operational\n",
      "Aug 20, 2025\n",
      "30 days ago\n",
      "60 days ago\n",
      "90 days ago\n",
      "Today\n",
      "Powered by\n",
      "Better Stack\n",
      "\n",
      "\n",
      "\n",
      "changelog\n",
      "Webpage Title:\n",
      "Changelog - Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Changelog\n",
      "Keep track of latest changes on the Hugging Face Hub\n",
      "Jul 30, 25\n",
      "Upvote\n",
      "114\n",
      "+109\n",
      "Jul 30, 25\n",
      "Introducing HF Jobs: Run scalable compute jobs on Hugging Face\n",
      "Upvote\n",
      "114\n",
      "+109\n",
      "Hugging Face Jobs lets you effortlessly run compute tasks on our infrastructure‚Äîfrom simple scripts to large-scale workloads‚Äîwith a simple CLI. Whether you need CPUs, or high-end GPUs, Jobs provides instant access to the hardware you need, billed by the second.\n",
      "Quick Start Examples:\n",
      "Run Python code directly:\n",
      "hf\n",
      "jobs\n",
      "run python:3.12 python -c\n",
      "\"print('Hello from the cloud!')\"\n",
      "Use GPUs without any setup:\n",
      "hf\n",
      "jobs\n",
      "run --flavor=t4-small ubuntu nvidia-smi\n",
      "Jul 28, 25\n",
      "Upvote\n",
      "64\n",
      "+59\n",
      "Jul 28, 25\n",
      "Trending Papers\n",
      "Upvote\n",
      "64\n",
      "+59\n",
      "The Daily Papers page now includes\n",
      "Trending Papers\n",
      ", showcasing the most popular and impactful research papers from the community, along with their corresponding code implementations on GitHub. Trending Papers are ranked based on recent GitHub star activity.\n",
      "Jul 25, 25\n",
      "Upvote\n",
      "64\n",
      "+59\n",
      "Jul 25, 25\n",
      "Introducing a better Hugging Face CLI\n",
      "Upvote\n",
      "64\n",
      "+59\n",
      "We‚Äôve renamed\n",
      "huggingface-cli\n",
      "to\n",
      "hf\n",
      "and overhauled the command structure for speed and clarity. The new CLI now uses the format\n",
      "hf <resource> <action>\n",
      ", so commands like\n",
      "hf auth login\n",
      ",\n",
      "hf repo create\n",
      ", or\n",
      "hf download Qwen/Qwen3-0.6B\n",
      "are now consistent and intuitive.\n",
      "Migration is easy, the old CLI still works and will gently point you to the new commands.\n",
      "You can learn more by reading the full announcement in this\n",
      "blog article\n",
      ".\n",
      "Jul 23, 25\n",
      "Upvote\n",
      "37\n",
      "+32\n",
      "Jul 23, 25\n",
      "JSON Support in the Dataset Viewer\n",
      "Upvote\n",
      "37\n",
      "+32\n",
      "You can now view JSON cells directly in the Dataset Viewer, making it far easier to expand nested structures, copy useful snippets, and understand your data.\n",
      "This feature is particularly useful for tool calling datasets like\n",
      "interstellarninja/hermes_reasoning_tool_use\n",
      "and\n",
      "Salesforce/APIGen-MT-5k\n",
      ".\n",
      "Jul 18, 25\n",
      "Upvote\n",
      "79\n",
      "+74\n",
      "Jul 18, 25\n",
      "Inference Providers now fully support OpenAI-compatible API\n",
      "Upvote\n",
      "79\n",
      "+74\n",
      "Our Inference Providers service now fully supports the OpenAI-compatible API, making it easy to integrate with existing workflows. Plus, you can now specify the provider name directly in the model path for greater flexibility.\n",
      "This means you can effortlessly switch between different inference providers while using the familiar OpenAI client‚Äîjust point to\n",
      "router.huggingface.co/v1\n",
      "and include the provider in the model name.\n",
      "This update simplifies integration while maintaining full compatibility with OpenAI's SDK. Try it now with supported providers like novita, groq, together, and more!\n",
      "Previous\n",
      "1\n",
      "2\n",
      "3\n",
      "Next\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "Company page\n",
      "Webpage Title:\n",
      "Brand assets - Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Hugging Face ¬∑ Brand assets\n",
      "HF Logos\n",
      ".svg\n",
      ".png\n",
      ".ai\n",
      ".svg\n",
      ".png\n",
      ".ai\n",
      ".svg\n",
      ".png\n",
      ".ai\n",
      "HF Colors\n",
      "#FFD21E\n",
      "#FF9D00\n",
      "#6B7280\n",
      "HF Bio\n",
      "Hugging Face is the collaboration platform for the machine learning community.\n",
      "\n",
      "The Hugging Face Hub works as a central place where anyone can share, explore, discover, and experiment with open-source ML. HF empowers the next generation of machine learning engineers, scientists, and end users to learn, collaborate and share their work to build an open and ethical AI future together.\n",
      "\n",
      "With the fast-growing community, some of the most used open-source ML libraries and tools, and a talented science team exploring the edge of tech, Hugging Face is at the heart of the AI revolution.\n",
      "Copy to clipboard\n",
      "HF Universe\n",
      "Find other assets available for use from the Hugging Face brand universe\n",
      "here\n",
      ".\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Changelog\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "GitHub page\n",
      "Webpage Title:\n",
      "Hugging Face ¬∑ GitHub\n",
      "Webpage Contents:\n",
      "Skip to content\n",
      "Navigation Menu\n",
      "Toggle navigation\n",
      "Sign in\n",
      "Appearance settings\n",
      "huggingface\n",
      "Product\n",
      "GitHub Copilot\n",
      "Write better code with AI\n",
      "GitHub Spark\n",
      "New\n",
      "Build and deploy intelligent apps\n",
      "GitHub Models\n",
      "New\n",
      "Manage and compare prompts\n",
      "GitHub Advanced Security\n",
      "Find and fix vulnerabilities\n",
      "Actions\n",
      "Automate any workflow\n",
      "Codespaces\n",
      "Instant dev environments\n",
      "Issues\n",
      "Plan and track work\n",
      "Code Review\n",
      "Manage code changes\n",
      "Discussions\n",
      "Collaborate outside of code\n",
      "Code Search\n",
      "Find more, search less\n",
      "Explore\n",
      "Why GitHub\n",
      "All features\n",
      "Documentation\n",
      "GitHub Skills\n",
      "Blog\n",
      "Solutions\n",
      "By company size\n",
      "Enterprises\n",
      "Small and medium teams\n",
      "Startups\n",
      "Nonprofits\n",
      "By use case\n",
      "DevSecOps\n",
      "DevOps\n",
      "CI/CD\n",
      "View all use cases\n",
      "By industry\n",
      "Healthcare\n",
      "Financial services\n",
      "Manufacturing\n",
      "Government\n",
      "View all industries\n",
      "View all solutions\n",
      "Resources\n",
      "Topics\n",
      "AI\n",
      "DevOps\n",
      "Security\n",
      "Software Development\n",
      "View all\n",
      "Explore\n",
      "Learning Pathways\n",
      "Events & Webinars\n",
      "Ebooks & Whitepapers\n",
      "Customer Stories\n",
      "Partners\n",
      "Executive Insights\n",
      "Open Source\n",
      "GitHub Sponsors\n",
      "Fund open source developers\n",
      "The ReadME Project\n",
      "GitHub community articles\n",
      "Repositories\n",
      "Topics\n",
      "Trending\n",
      "Collections\n",
      "Enterprise\n",
      "Enterprise platform\n",
      "AI-powered developer platform\n",
      "Available add-ons\n",
      "GitHub Advanced Security\n",
      "Enterprise-grade security features\n",
      "Copilot for business\n",
      "Enterprise-grade AI features\n",
      "Premium Support\n",
      "Enterprise-grade 24/7 support\n",
      "Pricing\n",
      "Search or jump to...\n",
      "Search code, repositories, users, issues, pull requests...\n",
      "Search\n",
      "Clear\n",
      "Search syntax tips\n",
      "Provide feedback\n",
      "We read every piece of feedback, and take your input very seriously.\n",
      "Include my email address so I can be contacted\n",
      "Cancel\n",
      "Submit feedback\n",
      "Saved searches\n",
      "Use saved searches to filter your results more quickly\n",
      "Cancel\n",
      "Create saved search\n",
      "Sign in\n",
      "Sign up\n",
      "Appearance settings\n",
      "Resetting focus\n",
      "You signed in with another tab or window.\n",
      "Reload\n",
      "to refresh your session.\n",
      "You signed out in another tab or window.\n",
      "Reload\n",
      "to refresh your session.\n",
      "You switched accounts on another tab or window.\n",
      "Reload\n",
      "to refresh your session.\n",
      "Dismiss alert\n",
      "Hugging Face\n",
      "The AI community building the future.\n",
      "Verified\n",
      "We've verified that the organization\n",
      "huggingface\n",
      "controls the domain:\n",
      "huggingface.co\n",
      "Learn more about verified organizations\n",
      "Sponsor\n",
      "52.9k\n",
      "followers\n",
      "NYC + Paris\n",
      "https://huggingface.co/\n",
      "X\n",
      "@huggingface\n",
      "Overview\n",
      "Repositories\n",
      "Projects\n",
      "Packages\n",
      "People\n",
      "Sponsoring\n",
      "1\n",
      "More\n",
      "Overview\n",
      "Repositories\n",
      "Projects\n",
      "Packages\n",
      "People\n",
      "Sponsoring\n",
      "Pinned\n",
      "Loading\n",
      "transformers\n",
      "transformers\n",
      "Public\n",
      "ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.\n",
      "Python\n",
      "149k\n",
      "30.1k\n",
      "diffusers\n",
      "diffusers\n",
      "Public\n",
      "ü§ó Diffusers: State-of-the-art diffusion models for image, video, and audio generation in PyTorch and FLAX.\n",
      "Python\n",
      "30.4k\n",
      "6.2k\n",
      "datasets\n",
      "datasets\n",
      "Public\n",
      "ü§ó The largest hub of ready-to-use datasets for AI models with fast, easy-to-use and efficient data manipulation tools\n",
      "Python\n",
      "20.5k\n",
      "2.9k\n",
      "peft\n",
      "peft\n",
      "Public\n",
      "ü§ó PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.\n",
      "Python\n",
      "19.4k\n",
      "2k\n",
      "accelerate\n",
      "accelerate\n",
      "Public\n",
      "üöÄ A simple way to launch, train, and use PyTorch models on almost any device and distributed configuration, automatic mixed precision (including fp8), and easy-to-configure FSDP and DeepSpeed support\n",
      "Python\n",
      "9.1k\n",
      "1.2k\n",
      "optimum\n",
      "optimum\n",
      "Public\n",
      "üöÄ Accelerate inference and training of ü§ó Transformers, Diffusers, TIMM and Sentence Transformers with easy to use hardware optimization tools\n",
      "Python\n",
      "3k\n",
      "573\n",
      "Repositories\n",
      "Loading\n",
      "Type\n",
      "Select type\n",
      "Forks\n",
      "Archived\n",
      "Mirrors\n",
      "Templates\n",
      "Language\n",
      "Select language\n",
      "All\n",
      "C\n",
      "C#\n",
      "C++\n",
      "Cuda\n",
      "Dockerfile\n",
      "Go\n",
      "Handlebars\n",
      "HTML\n",
      "Java\n",
      "JavaScript\n",
      "Jupyter Notebook\n",
      "Kotlin\n",
      "Lua\n",
      "Makefile\n",
      "MDX\n",
      "Mustache\n",
      "Nix\n",
      "Python\n",
      "Rust\n",
      "Shell\n",
      "Smarty\n",
      "Svelte\n",
      "Swift\n",
      "TypeScript\n",
      "Sort\n",
      "Select order\n",
      "Last updated\n",
      "Name\n",
      "Stars\n",
      "Showing 10 of 341 repositories\n",
      "trl\n",
      "Public\n",
      "Train transformer language models with reinforcement learning.\n",
      "huggingface/trl‚Äôs past year of commit activity\n",
      "Python\n",
      "15,182\n",
      "Apache-2.0\n",
      "2,129\n",
      "462\n",
      "65\n",
      "Updated\n",
      "Aug 21, 2025\n",
      "boomtitan\n",
      "Public\n",
      "fork of torchtitan for the boom project\n",
      "huggingface/boomtitan‚Äôs past year of commit activity\n",
      "Python\n",
      "4\n",
      "BSD-3-Clause\n",
      "2\n",
      "0\n",
      "2\n",
      "Updated\n",
      "Aug 21, 2025\n",
      "transformers\n",
      "Public\n",
      "ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.\n",
      "huggingface/transformers‚Äôs past year of commit activity\n",
      "Python\n",
      "148,579\n",
      "Apache-2.0\n",
      "30,115\n",
      "1,082\n",
      "(2 issues need help)\n",
      "876\n",
      "Updated\n",
      "Aug 20, 2025\n",
      "optimum\n",
      "Public\n",
      "üöÄ Accelerate inference and training of ü§ó Transformers, Diffusers, TIMM and Sentence Transformers with easy to use hardware optimization tools\n",
      "huggingface/optimum‚Äôs past year of commit activity\n",
      "Python\n",
      "3,032\n",
      "Apache-2.0\n",
      "573\n",
      "260\n",
      "(2 issues need help)\n",
      "23\n",
      "Updated\n",
      "Aug 20, 2025\n",
      "optimum-intel\n",
      "Public\n",
      "ü§ó Optimum Intel: Accelerate inference with Intel optimization tools\n",
      "huggingface/optimum-intel‚Äôs past year of commit activity\n",
      "Jupyter Notebook\n",
      "483\n",
      "Apache-2.0\n",
      "142\n",
      "33\n",
      "27\n",
      "Updated\n",
      "Aug 20, 2025\n",
      "nanoVLM\n",
      "Public\n",
      "The simplest, fastest repository for training/finetuning small-sized VLMs.\n",
      "huggingface/nanoVLM‚Äôs past year of commit activity\n",
      "Python\n",
      "3,915\n",
      "Apache-2.0\n",
      "363\n",
      "21\n",
      "11\n",
      "Updated\n",
      "Aug 20, 2025\n",
      "smolagents\n",
      "Public\n",
      "ü§ó smolagents: a barebones library for agents that think in code.\n",
      "huggingface/smolagents‚Äôs past year of commit activity\n",
      "Python\n",
      "22,255\n",
      "Apache-2.0\n",
      "1,954\n",
      "165\n",
      "(1 issue needs help)\n",
      "114\n",
      "Updated\n",
      "Aug 20, 2025\n",
      "ai-deadlines\n",
      "Public\n",
      "‚è∞ AI conference deadline countdowns\n",
      "huggingface/ai-deadlines‚Äôs past year of commit activity\n",
      "TypeScript\n",
      "278\n",
      "MIT\n",
      "50\n",
      "2\n",
      "0\n",
      "Updated\n",
      "Aug 20, 2025\n",
      "notebooks\n",
      "Public\n",
      "Notebooks using the Hugging Face libraries ü§ó\n",
      "huggingface/notebooks‚Äôs past year of commit activity\n",
      "Jupyter Notebook\n",
      "4,268\n",
      "Apache-2.0\n",
      "1,724\n",
      "150\n",
      "74\n",
      "Updated\n",
      "Aug 20, 2025\n",
      "datasets\n",
      "Public\n",
      "ü§ó The largest hub of ready-to-use datasets for AI models with fast, easy-to-use and efficient data manipulation tools\n",
      "huggingface/datasets‚Äôs past year of commit activity\n",
      "Python\n",
      "20,531\n",
      "Apache-2.0\n",
      "2,903\n",
      "843\n",
      "(2 issues need help)\n",
      "112\n",
      "Updated\n",
      "Aug 20, 2025\n",
      "View all repositories\n",
      "People\n",
      "View all\n",
      "Sponsoring\n",
      "Top languages\n",
      "Python\n",
      "Jupyter Notebook\n",
      "TypeScript\n",
      "Rust\n",
      "MDX\n",
      "Most used topics\n",
      "pytorch\n",
      "machine-learning\n",
      "nlp\n",
      "transformers\n",
      "deep-learning\n",
      "GitHub Sponsor\n",
      "Footer\n",
      "¬© 2025 GitHub,¬†Inc.\n",
      "Footer navigation\n",
      "Terms\n",
      "Privacy\n",
      "Security\n",
      "Status\n",
      "Docs\n",
      "Contact\n",
      "Manage cookies\n",
      "Do not share my personal information\n",
      "You can‚Äôt perform that action at this time.\n",
      "\n",
      "\n",
      "\n",
      "Twitter page\n",
      "Webpage Title:\n",
      "No title found\n",
      "Webpage Contents:\n",
      "JavaScript is not available.\n",
      "We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.\n",
      "Help Center\n",
      "Terms of Service\n",
      "Privacy Policy\n",
      "Cookie Policy\n",
      "Imprint\n",
      "Ads info\n",
      "¬© 2025 X Corp.\n",
      "Something went wrong, but don‚Äôt fret ‚Äî let‚Äôs give it another shot.\n",
      "Try again\n",
      "Some privacy related extensions may cause issues on x.com. Please disable them and try again.\n",
      "\n",
      "\n",
      "\n",
      "LinkedIn Company page\n",
      "Webpage Title:\n",
      "Hugging Face | LinkedIn\n",
      "Webpage Contents:\n",
      "Skip to main content\n",
      "LinkedIn\n",
      "Top Content\n",
      "People\n",
      "Learning\n",
      "Jobs\n",
      "Games\n",
      "Get the app\n",
      "Join now\n",
      "Sign in\n",
      "Hugging Face\n",
      "Software Development\n",
      "The AI community building the future.\n",
      "See jobs\n",
      "Follow\n",
      "View all 606 employees\n",
      "Report this company\n",
      "About us\n",
      "The AI community building the future.\n",
      "Website\n",
      "https://huggingface.co\n",
      "External link for Hugging Face\n",
      "Industry\n",
      "Software Development\n",
      "Company size\n",
      "51-200 employees\n",
      "Type\n",
      "Privately Held\n",
      "Founded\n",
      "2016\n",
      "Specialties\n",
      "machine learning, natural language processing, and deep learning\n",
      "Products\n",
      "Hugging Face\n",
      "Hugging Face\n",
      "Natural Language Processing (NLP) Software\n",
      "We‚Äôre on a journey to solve and democratize artificial intelligence through natural language.\n",
      "Locations\n",
      "Primary\n",
      "Get directions\n",
      "Paris, FR\n",
      "Get directions\n",
      "Employees at Hugging Face\n",
      "Ludovic Huraux\n",
      "Rajat Arya\n",
      "Tech Lead & Software Engineer @ HF | prev: co-founder XetHub, Apple, Turi, AWS, Microsoft\n",
      "Jeff Boudier\n",
      "Product + Growth at Hugging Face\n",
      "Terrence Rohan\n",
      "Seed Investor\n",
      "See all employees\n",
      "Updates\n",
      "Hugging Face\n",
      "1,041,199 followers\n",
      "12h\n",
      "Report this post\n",
      "Sun's out, models out. üòé\n",
      "IBM\n",
      "&\n",
      "NASA - National Aeronautics and Space Administration\n",
      "just released Surya on the Hub, an open-source heliophysics model trained on 14 years of observations from NASA‚Äôs Solar Dynamics Observatory, helping protect critical infra from space weather.\n",
      "https://lnkd.in/emV7XQvE\n",
      "257\n",
      "18 Comments\n",
      "Like\n",
      "Comment\n",
      "Share\n",
      "Hugging Face\n",
      "reposted this\n",
      "Brigitte Tousignant\n",
      "Comms ü§ó\n",
      "12h\n",
      "Report this post\n",
      "Meet Surya: the newest open-source model from\n",
      "IBM\n",
      "and\n",
      "NASA - National Aeronautics and Space Administration\n",
      "that helps predict solar storms before they fry satellites or knock out the grid. And it‚Äôs free for researchers, builders, and guardians of critical infrastructure on\n",
      "Hugging Face\n",
      ". \n",
      "\n",
      "Open source. Mission critical. Beautifully nerdy.\n",
      "https://lnkd.in/eSBjD5a3\n",
      "78\n",
      "Like\n",
      "Comment\n",
      "Share\n",
      "Hugging Face\n",
      "reposted this\n",
      "Daniel Vila Suero\n",
      "Building data tools @ Hugging Face ü§ó\n",
      "15h\n",
      "Report this post\n",
      "Hugging Face Unveils Al Sheets: A Free, Open-Source No-Code Toolkit for LLM-Powered Datasets üî•üî•üî•üî•\n",
      "\n",
      "\n",
      "Very nice coverage!\n",
      "https://lnkd.in/ddsjZxaR\n",
      "303\n",
      "11 Comments\n",
      "Like\n",
      "Comment\n",
      "Share\n",
      "Hugging Face\n",
      "reposted this\n",
      "Sayak Paul\n",
      "ML @ Hugging Face ü§ó\n",
      "1d\n",
      "Report this post\n",
      "New Diffusers release is out.\n",
      "\n",
      "Really great to see the state of things in image edits, video fidelity being pushed further and further, thanks to the community!\n",
      "\n",
      "This release also features new fine-tuning scripts for Qwen-Image and Flux Kontext (with support for image inputs). So, get busy making these models your own ü§ó\n",
      "\n",
      "We also improved the loading speed of Diffusers pipelines and models. This will become particularly evident when operating with large models like Wan, Qwen, etc.\n",
      "\n",
      "As always, don't forget to check out the release notes for the full disclosure. I will leave the link in the comments.\n",
      "215\n",
      "2 Comments\n",
      "Like\n",
      "Comment\n",
      "Share\n",
      "Hugging Face\n",
      "reposted this\n",
      "Leandro von Werra\n",
      "Head of Research at Hugging Face\n",
      "1d\n",
      "Report this post\n",
      "Excited to release: Jupyter Agent 2\n",
      "\n",
      "The agent can load data, execute code, plot results inside Jupyter faster than you can scroll!\n",
      "\n",
      "ü§ñ Powered by Qwen3-Coder\n",
      "‚ö°Ô∏è Running on Cerebras\n",
      "‚öôÔ∏è Executed in E2B\n",
      "‚ÜïÔ∏è Upload your files\n",
      "\n",
      "All videos are in *real time*!\n",
      "https://lnkd.in/en3bQMyc\n",
      "‚Ä¶more\n",
      "1,134\n",
      "52 Comments\n",
      "Like\n",
      "Comment\n",
      "Share\n",
      "Hugging Face\n",
      "reposted this\n",
      "Sayak Paul\n",
      "ML @ Hugging Face ü§ó\n",
      "1d\n",
      "Report this post\n",
      "New Diffusers release is out.\n",
      "\n",
      "Really great to see the state of things in image edits, video fidelity being pushed further and further, thanks to the community!\n",
      "\n",
      "This release also features new fine-tuning scripts for Qwen-Image and Flux Kontext (with support for image inputs). So, get busy making these models your own ü§ó\n",
      "\n",
      "We also improved the loading speed of Diffusers pipelines and models. This will become particularly evident when operating with large models like Wan, Qwen, etc.\n",
      "\n",
      "As always, don't forget to check out the release notes for the full disclosure. I will leave the link in the comments.\n",
      "215\n",
      "2 Comments\n",
      "Like\n",
      "Comment\n",
      "Share\n",
      "Hugging Face\n",
      "reposted this\n",
      "Yoni Gozlan\n",
      "ML Engineer @Hugging Face ü§ó\n",
      "2d\n",
      "Report this post\n",
      "üöÄ Better late than never, SAM2 is now available in\n",
      "Hugging Face\n",
      "ü§ó Transformers!\n",
      "\n",
      "introduced by\n",
      "Meta\n",
      "FAIR, Segment Anything Model 2 (SAM2) is the natural evolution of the first SAM: extending promptable segmentation from static images into the video domain. With its streaming memory architecture, SAM2 can track and refine object masks across long videos in real time, making it one of the most powerful models for interactive and automatic video segmentation.\n",
      "\n",
      "The model builds a memory bank of past frames and prompts, encoding both spatial features and compact ‚Äúobject pointer‚Äù tokens that summarize the target. Each new frame is then processed in relation to this memory through attention, letting SAM2 recall what the object looked like and where it was last observed. Combined with an explicit occlusion prediction head, this design allows SAM2 to keep track of objects robustly and to recover them with minimal user input after partial or full occlusion.\n",
      "\n",
      "Many thanks to the team at Meta FAIR for developing and releasing such a foundational model for vision. I am happy to have worked on bringing SAM2 into Transformers together with\n",
      "Daniel Choi\n",
      ", making it easier than ever for the open-source community to use.\n",
      "\n",
      "üëâ Try SAM2 video tracking directly in this Hugging Face Space (the demo video below was made with it!):\n",
      "https://lnkd.in/e-UR8t_N\n",
      "* ü§ó SAM2.1 Large checkpoint:\n",
      "https://lnkd.in/eEQ_YyVT\n",
      "* ü§ó Transformers docs (SAM2):\n",
      "https://lnkd.in/eZ2gAUfh\n",
      "* ü§ó Transformers docs (SAM2 Video):\n",
      "https://lnkd.in/e3vuJxjM\n",
      "‚Ä¶more\n",
      "788\n",
      "17 Comments\n",
      "Like\n",
      "Comment\n",
      "Share\n",
      "Hugging Face\n",
      "reposted this\n",
      "Vaibhav Srivastav\n",
      "whatever needs doing @ Hugging Face\n",
      "3d\n",
      "Report this post\n",
      "NVIDIA ON A ROLL! Canary 1B and Parakeet TDT (0.6B) SoTA ASR models - Multilingual, Open Source üî•\n",
      "\n",
      "- 1B and 600M parameters\n",
      "- 25 languages\n",
      "- automatic language detection and translation\n",
      "- word and sentence timestamps\n",
      "- transcribe up to 3 hours of audio in one go\n",
      "- trained on 1 Million hours of data\n",
      "- SoTA on Open ASR Leaderboard\n",
      "\n",
      "- CC-BY licensed üí•\n",
      "\n",
      "Available on Hugging Face, go check them out today!\n",
      "460\n",
      "37 Comments\n",
      "Like\n",
      "Comment\n",
      "Share\n",
      "Hugging Face\n",
      "reposted this\n",
      "Daniel Vila Suero\n",
      "Building data tools @ Hugging Face ü§ó\n",
      "4d\n",
      "Report this post\n",
      "You can now use\n",
      "Hugging Face\n",
      "to generate and transform data at scale, no GPU required \n",
      "\n",
      "\n",
      "Thanks to HF Jobs, AI Sheets and vLLM!\n",
      "\n",
      "\n",
      "Here's the recipe:\n",
      "\n",
      "1. go to\n",
      "https://lnkd.in/gdKeV-zW\n",
      "and import a dataset from the hub \n",
      "\n",
      "2. add the columns you want (e.g., classifying text, generating responses to prompts, etc)\n",
      "\n",
      "3. push the dataset to the hub\n",
      "\n",
      "4. run this job script\n",
      "https://lnkd.in/diYnnY8J\n",
      "(see how to run it in the first comment)\n",
      "GitHub - huggingface/aisheets: Build, enrich, and transform datasets using AI models with no code\n",
      "github.com\n",
      "243\n",
      "5 Comments\n",
      "Like\n",
      "Comment\n",
      "Share\n",
      "Hugging Face\n",
      "reposted this\n",
      "Vaibhav Srivastav\n",
      "whatever needs doing @ Hugging Face\n",
      "1w\n",
      "Report this post\n",
      "OpenAI gpt-oss 120B orchestrates a full video using Hugging Face space! ü§Ø\n",
      "\n",
      "All of it, in one SINGLE prompt:¬†\n",
      "create an image of a Labrador and use it to generate a simple video of it \n",
      "\n",
      "Full guide and steps here:\n",
      "https://lnkd.in/g8bhzR_e\n",
      "üõ†Ô∏è Tools used:\n",
      "1. Flux.1 Krea Dev by\n",
      "Black Forest Labs\n",
      "2. LTX Fast by\n",
      "Lightricks\n",
      "That's it, gpt-oss 120B is one of the BEST open source models I've used for tool calling so far! Kudos\n",
      "OpenAI\n",
      "ü§ó\n",
      "‚Ä¶more\n",
      "651\n",
      "31 Comments\n",
      "Like\n",
      "Comment\n",
      "Share\n",
      "Join now to see what you are missing\n",
      "Find people you know at Hugging Face\n",
      "Browse recommended jobs for you\n",
      "View all updates, news, and articles\n",
      "Join now\n",
      "Similar pages\n",
      "Anthropic\n",
      "Research Services\n",
      "Perplexity\n",
      "Software Development\n",
      "San Francisco, California\n",
      "Mistral AI\n",
      "Technology, Information and Internet\n",
      "Paris, France\n",
      "OpenAI\n",
      "Research Services\n",
      "San Francisco, CA\n",
      "LangChain\n",
      "Technology, Information and Internet\n",
      "Generative AI\n",
      "Technology, Information and Internet\n",
      "Google DeepMind\n",
      "Research Services\n",
      "London, London\n",
      "DeepLearning.AI\n",
      "Software Development\n",
      "Palo Alto, California\n",
      "Cohere\n",
      "Software Development\n",
      "Toronto, Ontario\n",
      "LlamaIndex\n",
      "Technology, Information and Internet\n",
      "San Francisco, California\n",
      "Show more similar pages\n",
      "Show fewer similar pages\n",
      "Browse jobs\n",
      "Engineer jobs\n",
      "555,845 open jobs\n",
      "Machine Learning Engineer jobs\n",
      "148,937 open jobs\n",
      "Scientist jobs\n",
      "48,969 open jobs\n",
      "Software Engineer jobs\n",
      "300,699 open jobs\n",
      "Analyst jobs\n",
      "694,057 open jobs\n",
      "Intern jobs\n",
      "71,196 open jobs\n",
      "Developer jobs\n",
      "258,935 open jobs\n",
      "Manager jobs\n",
      "1,880,925 open jobs\n",
      "Product Manager jobs\n",
      "199,941 open jobs\n",
      "Director jobs\n",
      "1,220,357 open jobs\n",
      "Python Developer jobs\n",
      "46,642 open jobs\n",
      "Data Scientist jobs\n",
      "264,158 open jobs\n",
      "Data Analyst jobs\n",
      "329,009 open jobs\n",
      "Senior Software Engineer jobs\n",
      "78,145 open jobs\n",
      "Project Manager jobs\n",
      "253,048 open jobs\n",
      "Researcher jobs\n",
      "195,654 open jobs\n",
      "Associate jobs\n",
      "1,091,945 open jobs\n",
      "Data Engineer jobs\n",
      "192,126 open jobs\n",
      "Vice President jobs\n",
      "235,270 open jobs\n",
      "Specialist jobs\n",
      "768,666 open jobs\n",
      "Show more jobs like this\n",
      "Show fewer jobs like this\n",
      "Funding\n",
      "Hugging Face\n",
      "8 total rounds\n",
      "Last Round\n",
      "Series unknown\n",
      "Sep 1, 2024\n",
      "External Crunchbase Link for last round of funding\n",
      "See more info on\n",
      "crunchbase\n",
      "More searches\n",
      "More searches\n",
      "Engineer jobs\n",
      "Scientist jobs\n",
      "Machine Learning Engineer jobs\n",
      "Software Engineer jobs\n",
      "Intern jobs\n",
      "Developer jobs\n",
      "Analyst jobs\n",
      "Manager jobs\n",
      "Senior Software Engineer jobs\n",
      "Data Scientist jobs\n",
      "Researcher jobs\n",
      "Product Manager jobs\n",
      "Director jobs\n",
      "Associate jobs\n",
      "Intelligence Specialist jobs\n",
      "Data Analyst jobs\n",
      "Data Science Specialist jobs\n",
      "Python Developer jobs\n",
      "Quantitative Analyst jobs\n",
      "Project Manager jobs\n",
      "Account Executive jobs\n",
      "Specialist jobs\n",
      "Data Engineer jobs\n",
      "Designer jobs\n",
      "Quantitative Researcher jobs\n",
      "Consultant jobs\n",
      "Solutions Architect jobs\n",
      "Vice President jobs\n",
      "User Experience Designer jobs\n",
      "Head jobs\n",
      "Full Stack Engineer jobs\n",
      "Engineering Manager jobs\n",
      "Software Engineer Intern jobs\n",
      "Junior Software Engineer jobs\n",
      "Software Intern jobs\n",
      "Product Designer jobs\n",
      "Solutions Engineer jobs\n",
      "Staff Software Engineer jobs\n",
      "Program Manager jobs\n",
      "Senior Scientist jobs\n",
      "Writer jobs\n",
      "Research Intern jobs\n",
      "Senior Product Manager jobs\n",
      "Summer Intern jobs\n",
      "Account Manager jobs\n",
      "Recruiter jobs\n",
      "Lead jobs\n",
      "Research Engineer jobs\n",
      "Computer Science Intern jobs\n",
      "Platform Engineer jobs\n",
      "Junior Developer jobs\n",
      "Android Developer jobs\n",
      "User Experience Researcher jobs\n",
      "Java Software Engineer jobs\n",
      "Site Reliability Engineer jobs\n",
      "Graduate jobs\n",
      "Software Engineering Manager jobs\n",
      "Representative jobs\n",
      "Business Development Specialist jobs\n",
      "Computer Engineer jobs\n",
      "LinkedIn\n",
      "¬© 2025\n",
      "About\n",
      "Accessibility\n",
      "User Agreement\n",
      "Privacy Policy\n",
      "Cookie Policy\n",
      "Copyright Policy\n",
      "Brand Policy\n",
      "Guest Controls\n",
      "Community Guidelines\n",
      "ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)\n",
      "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ (Bangla)\n",
      "ƒåe≈°tina (Czech)\n",
      "Dansk (Danish)\n",
      "Deutsch (German)\n",
      "ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ (Greek)\n",
      "English (English)\n",
      "Espa√±ol (Spanish)\n",
      "ŸÅÿßÿ±ÿ≥€å (Persian)\n",
      "Suomi (Finnish)\n",
      "Fran√ßais (French)\n",
      "‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi)\n",
      "Magyar (Hungarian)\n",
      "Bahasa Indonesia (Indonesian)\n",
      "Italiano (Italian)\n",
      "◊¢◊ë◊®◊ô◊™ (Hebrew)\n",
      "Êó•Êú¨Ë™û (Japanese)\n",
      "ÌïúÍµ≠Ïñ¥ (Korean)\n",
      "‡§Æ‡§∞‡§æ‡§†‡•Ä (Marathi)\n",
      "Bahasa Malaysia (Malay)\n",
      "Nederlands (Dutch)\n",
      "Norsk (Norwegian)\n",
      "‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä (Punjabi)\n",
      "Polski (Polish)\n",
      "Portugu√™s (Portuguese)\n",
      "Rom√¢nƒÉ (Romanian)\n",
      "–†—É—Å—Å–∫–∏–π (Russian)\n",
      "Svenska (Swedish)\n",
      "‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å (Telugu)\n",
      "‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai)\n",
      "Tagalog (Tagalog)\n",
      "T√ºrk√ße (Turkish)\n",
      "–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ (Ukrainian)\n",
      "Ti·∫øng Vi·ªát (Vietnamese)\n",
      "ÁÆÄ‰Ωì‰∏≠Êñá (Chinese (Simplified))\n",
      "Ê≠£È´î‰∏≠Êñá (Chinese (Traditional))\n",
      "Language\n",
      "Agree & Join LinkedIn\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "Sign in to see who you already know at Hugging Face\n",
      "Sign in\n",
      "Welcome back\n",
      "Email or phone\n",
      "Password\n",
      "Show\n",
      "Forgot password?\n",
      "Sign in\n",
      "or\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "or\n",
      "New to LinkedIn?\n",
      "Join now\n",
      "By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs\n",
      "User Agreement\n",
      ",\n",
      "Privacy Policy\n",
      ", and\n",
      "Cookie Policy\n",
      ".\n",
      "LinkedIn\n",
      "LinkedIn is better on the app\n",
      "Don‚Äôt have the app? Get it in the Microsoft Store.\n",
      "Open the app\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_all_details(\"https://huggingface.co\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b863a55-f86c-4e3f-8a79-94e24c1a8cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of several relevant pages from a company website \\\n",
    "and creates a short brochure about the company for prospective customers, investors and recruits. Respond in markdown.\\\n",
    "Include details of company culture, customers and careers/jobs if you have the information.\"\n",
    "\n",
    "# Or uncomment the lines below for a more humorous brochure - this demonstrates how easy it is to incorporate 'tone':\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of several relevant pages from a company website \\\n",
    "and creates a short humorous, entertaining, jokey brochure about the company for prospective customers, investors and recruits. Respond in markdown.\\\n",
    "Include details of company culture, customers and careers/jobs if you have the information.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ab83d92-d36b-4ce0-8bcc-5bb4c2f8ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brochure_user_prompt(company_name, url):\n",
    "    user_prompt = f\"You are looking at a company called: {company_name}\\n\"\n",
    "    user_prompt += f\"Here are the contents of its landing page and other relevant pages; use this information to build a short brochure of the company in markdown.\\n\"\n",
    "    user_prompt += get_all_details(url)\n",
    "    user_prompt = user_prompt[:5_000] # Truncate if more than 5,000 characters\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd909e0b-1312-4ce2-a553-821e795d7572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links: {'links': [{'type': 'About page', 'url': 'https://blog.huggingface.co/'}, {'type': 'Company/Brand page', 'url': 'https://brand.huggingface.co/'}, {'type': 'Changelog', 'url': 'https://changelog.huggingface.co/'}, {'type': 'Docs', 'url': 'https://docs.huggingface.co/'}]}\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='blog.huggingface.co', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000022BD1FE79E0>: Failed to resolve 'blog.huggingface.co' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mgaierror\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\util\\connection.py:60\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, label empty or too long\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     61\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:978\u001b[39m, in \u001b[36mgetaddrinfo\u001b[39m\u001b[34m(host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m    977\u001b[39m addrlist = []\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    979\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[31mgaierror\u001b[39m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNameResolutionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    752\u001b[39m sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\connection.py:205\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01me\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mNameResolutionError\u001b[39m: <urllib3.connection.HTTPSConnection object at 0x0000022BD1FE79E0>: Failed to resolve 'blog.huggingface.co' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\urllib3\\util\\retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='blog.huggingface.co', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000022BD1FE79E0>: Failed to resolve 'blog.huggingface.co' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_brochure_user_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHuggingFace\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://huggingface.co\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mget_brochure_user_prompt\u001b[39m\u001b[34m(company_name, url)\u001b[39m\n\u001b[32m      2\u001b[39m user_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are looking at a company called: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m user_prompt += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHere are the contents of its landing page and other relevant pages; use this information to build a short brochure of the company in markdown.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m user_prompt += \u001b[43mget_all_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m user_prompt = user_prompt[:\u001b[32m5_000\u001b[39m] \u001b[38;5;66;03m# Truncate if more than 5,000 characters\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m user_prompt\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mget_all_details\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m links[\u001b[33m\"\u001b[39m\u001b[33mlinks\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m      7\u001b[39m     result += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mlink[\u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     result += \u001b[43mWebsite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.get_contents()\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mWebsite.__init__\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, url):\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mself\u001b[39m.url = url\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mself\u001b[39m.body = response.content\n\u001b[32m     17\u001b[39m     soup = BeautifulSoup(\u001b[38;5;28mself\u001b[39m.body, \u001b[33m'\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\requests\\adapters.py:700\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _SSLError):\n\u001b[32m    697\u001b[39m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[32m    698\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    703\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n",
      "\u001b[31mConnectionError\u001b[39m: HTTPSConnectionPool(host='blog.huggingface.co', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000022BD1FE79E0>: Failed to resolve 'blog.huggingface.co' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "get_brochure_user_prompt(\"HuggingFace\", \"https://huggingface.co\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44de579-4a1a-4e6a-a510-20ea3e4b8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_brochure(company_name, url):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url)}\n",
    "          ],\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093444a-9407-42ae-924a-145730591a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_brochure(\"HuggingFace\", \"https://huggingface.co\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eaaab7-0b47-4b29-82d4-75d474ad8d18",
   "metadata": {},
   "source": [
    "## Finally - a minor improvement\n",
    "\n",
    "With a small adjustment, we can change this so that the results stream back from OpenAI,\n",
    "with the familiar typewriter animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db0e49-f261-4137-aabe-92dd601f7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_brochure(company_name, url):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url)}\n",
    "          ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf0ae3-ee9d-4a72-9cd6-edcac67ceb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_brochure(\"HuggingFace\", \"https://huggingface.co\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3f8d8-a3eb-41c8-b1aa-9f60686a653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try changing the system prompt to the humorous version when you make the Brochure for Hugging Face:\n",
    "\n",
    "stream_brochure(\"HuggingFace\", \"https://huggingface.co\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
